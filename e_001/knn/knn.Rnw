\documentclass[11pt]{article}

\usepackage{epsilonj}

\RequirePackage{graphicx}
\usepackage{verbatim}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\addbibresource{../../template/epsilon.bib}

\begin{document}

\TITLE{Метод $k$ ближайших соседей и распознавание рукописных цифр}
\SHORTTITLE{Метод $k$ ближайших соседей и распознавание рукописных цифр}
\AUTHOR{Саша Кузнецова}{НИУ ВШЭ, Москва.}
\SHORTAUTHOR{Саша Кузнецова}

\DoFirstPageTechnicalStuff

\begin{abstract}
Задача распознавания рукописного текста "--- одна из классических задач машинного обучения, к~решению которой применялось такое количество алгоритмов, что она успешно может быть использована в~качестве учебной.

В~этой заметке мы будем учиться распознавать написанные от руки цифры.
\end{abstract}

\begin{keyword}
k ближайших соседей, R, машинное обучение, распознавание образов
\end{keyword}


Мы обратимся к~одному из наиболее известных хранилищ, в~котором собраны обработанные изображения цифр, разберём, как эти данные оттуда извлекать, а~затем применим к~ним алгоритм $k$ ближайших соседей, используя \proglang{R}.

\section{База данных MNIST}

Источником данных нам послужит база «MNIST» (\cite{mnistdigits}), \url{http://yann.lecun.com/exdb/mnist/}, в~которой хранятся 70\,000 изображений цифр, написанных несколькими сотнями разных людей.
Каждая картинка в~этой базе обработана так, чтобы поместиться в~квадратик размером $28\times28$ пикселей.
Каждый пиксель представлен числом от~0 до~255, где 0~соответствует белому цвету, а~255 "--- чёрному.

Все изображения поделены на две части: 60\,000 относятся к~учебной выборке, а~10\,000 "--- к~тестовой.
По учебной выборке наш алгоритм будет настраивать свои параметры, а~по тестовой мы будем оценивать качество классификации.
Такое разбиение нужно для того, чтобы убедиться, что алгоритм не переобучился, то есть не настроен исключительно на учебные примеры и~способен правильно классифицировать новые для него изображения.

Каждое из изображений в~нашей задаче должно быть отнесено к~одному из десяти классов "--- это цифры от~0 до~9.
Для элементов обучающей выборки известно, к~какому классу они принадлежат, поэтому настройка параметров классифицирующего алгоритма относится к~области \textit{обучения с~учителем}.
Это значит, что процесс обучения использует известные ответы и~стремится за счёт выбора параметров сделать предсказания алгоритма максимально близкими к~правильным (кстати, таким образом мы обучали все наши алгоритмы в~курсе эконометрики).

Как говорится, «наивные студенты думали, что .csv"=файлы на деревьях, как булки, растут», но всё оказалось совсем не так.
Для того чтобы прочитать IDX-файл, в~котором хранятся данные с~изображениями цифр в~базе «MNIST», мне потребовались Google и~кандидат физико"=математических наук.
Проблема заключается в~том, что данные хранятся в~этой базе в~бинарном виде, и~их не открыть в~привычных нам приложениях.
Вот как это можно сделать в~\proglang{R}.

\par\medskip
(1) Скачав данные с~сайта, открываем файл с~учебной выборкой на чтение командой \code|file|:

<<>>=
to.read <- file("train-images.idx3-ubyte", "rb")
@

\par\medskip (2) Данные устроены таким образом, что в~самом начале мы читаем заголовок из четырёх чисел. Эти первые четыре числа содержат информацию о~размерах выборки, упомянутых выше. Считываем их из полученного объекта \code|to.read| с~помощью функции для чтения бинарных данных \code|readBin|.

<<>>=
readBin(to.read, integer(), n = 4, endian="big")
@

\par\medskip (3) Далее для каждой картинки следуют $28\times28$ байт, содержащие информацию о~цвете каждого пикселя (числа от~0 до 255) и~записанные из изображения построчно.
Получаем большой массив \code|TRAIN| из 60\,000 таблиц размером $28\times28$: берём первые 28 чисел и~кладём их в~первый столбец первой таблицы, продолжаем, пока не дойдём до 28-го столбца, затем приступаем к~следующей таблице "--- и~так до конца.

Нужно обратить внимание, что изначально данные были разложены в~строку, но мы только что разложили их по столбцам, для того чтобы позже было удобнее выводить рисунок.

<<warning=FALSE>>=
TRAIN <- array(data = NA, dim = c(28,28,60000))

for(i in 1:60000)
  {
TRAIN[,,i] <- matrix(readBin(to.read,integer(), size = 1,
    n = 28*28, signed = FALSE, endian = "big"), 28, 28);
  }
close(to.read)
@

\par\medskip (4) Перед тем как начинать работать с~данными и~оценивать по ним какие-либо модели, обычно бывает полезно взглянуть на них и~построить описательные статистики. В~данном случае для этой цели может послужить сама картинка.

<<>>=
layout(matrix(c(1:36), 6, 6, byrow = TRUE),
   widths = lcm(rep(2.5,36)), heights = lcm(rep(2.5,36)))
par(mar=c(0,0,0,0))

for(i in 1:36)
  {
image(TRAIN[,28:1,i])
  }
@

Нарисуем цифры по первым 36~таблицам из массива \code|TRAIN|.
Нужно обратить внимание, что, нарисуй мы сейчас всё как есть, мы бы получили перевёрнутые цифры (это можно проверить), потому что функция \code|image| будет соотносить первый столбец с~нулевой ординатой на картинке, но мы помним, что первый столбец соответствовал верхней строке рисунка.
Для того чтобы избежать этой проблемы, будем рисовать столбцы в~обратном порядке, от 28-го к~1-му: \code|TRAIN[,28:1,]|.

\par\medskip (5) Описанную выше процедуру повторяем, чтобы считать метки классов для учебной выборки.
Заголовок в~данном случае состоит из двух чисел, а~данные должны восприниматься \proglang{R}'ом не как числа, а~как категории: этого можно добиться с~помощью команды \code|as.factor|.

<<>>=
to.read <- file("train-labels.idx1-ubyte", "rb")
header <- readBin(to.read, integer(), n=2, endian="big")
Train_labels <- readBin(to.read, integer(), size = 1,
            n = 60000, signed = FALSE, endian="big")
Train_labels <- as.factor(Train_labels)
close(to.read)
@

\par\medskip
Итак, на данном этапе мы являемся обладателями учебной выборки (но умеем читать и~многое другое) и~великолепной картинки, что означает, что пора бы с~ними что-нибудь да~сделать.

\section{Алгоритм $k$ ближайших соседей}

Одним из простейших алгоритмов классификации является метод $k$ ближайших соседей (\ENGs{$k$ Nearest Neighbours}, kNN).
Основной идеей данного алгоритма является то, что объект, как правило, находится в~окружении объектов своего же класса.
Таким образом, чтобы классифицировать новый объект, мы должны посмотреть на $k$ ближайших к~нему объектов из учебной выборки и~отнести его к~тому классу, который среди них чаще встретился.

Первый вопрос, который перед нами встаёт, "--- это выбор функции расстояния между объектами выборки.
В~простом случае, когда объекты представлены в~виде числовых векторов, пользуются простой евклидовой метрикой, то есть $\rho(a, b) = \sqrt{\sum\limits_{k = 1}^m(a_k-b_k)^2}$.

Второй проблемой является выбор значения $k$: сколько объектов нам нужно посмотреть, чтобы наиболее точно классифицировать наш?
Единого правила для того, чтобы выбрать $k$, не существует, однако нужно учитывать, что при слишком маленьких $k$ алгоритм будет неустойчивым, а~при слишком больших начнёт подстраиваться к~шуму и~терять обобщающую способность.
Конкретное значение $k$ можно определить опытным путём: попробовать диапазон значений и~посмотреть, какое подходит лучше.

\subsection{Существующие реализации}

Для реализации алгоритма $k$ ближайших соседей в~\proglang{R} есть как минимум три пакета: \pkg{kknn}, \pkg{FNN} и~\pkg{RWeka}, причём в~двух последних есть ещё и~готовые базы данных.

Пакет \pkg{kknn} реализует алгоритм kNN с~весами, где голоса «соседей» не равнозначны, а~входят в~общую сумму с~определёнными весами. Мы его реализовывать не будем.

Воспользуемся функцией \code|install.packages(c("FNN", "RWeka"))|, чтобы установить пакеты.

\subsection{Пакет \pkg{FNN}}

Воспользуемся теперь пакетом \pkg{FNN} (\cite{RFNNpackage14}), предварительно проделав с~данными для тестовой выборки то же, что и~с~учебными, в~итоге получив два больших массива, \code|TRAIN| и~\code|TEST|, а~также два вектора ответов, \code|Trainlabels| и~\code|Testlabels|.

<<echo=FALSE>>=
to.read <- file("t10k-images.idx3-ubyte", "rb")
header <- readBin(to.read, integer(), n=4, endian="big")

TEST <- array(data = NA, dim = c(28,28,10000))

for(i in 1:10000)
  {
TEST[,,i] <- matrix(readBin(to.read,integer(), size=1, n=28*28,signed = FALSE, endian="big"),28,28);
  }
close(to.read)
to.read <- file("t10k-labels.idx1-ubyte", "rb")
header <- readBin(to.read, integer(), n=2, endian="big")

Test_labels <- readBin(to.read, integer(), size = 1, n=10000, signed = FALSE, endian="big")
close(to.read)
Test_labels <- as.factor(Test_labels)
@

Функция \code|knn| пакета \pkg{FNN} в~качестве аргументов принимает матрицы учебных и~тестовых данных, а~также вектор ответов для тренировочной выборки, а~на выходе отдаёт предсказанную классификацию для тестовой выборки.
Эта функция сразу и~обучается, и~предсказывает "--- очень удобно.

Сейчас наши данные выглядят не так, как нужно этой функции, поэтому преобразуем их в~матрицу, содержащую значения цвета всех пикселей, размещённых в~одну строку для каждой картинки.
Воспользуемся функцией \code|as.vector|, которая будет по очереди брать столбцы из матрицы и~выкладывать их в~одну строку.
Кроме того, для примера мы будем использовать только первую тысячу наблюдений из учебной выборки и~пятьсот из тестовой.
Если читатель желает получить более точный классификатор, то в~этом месте следует использовать все 60\,000 и~10\,000 соответственно.

<<warning=FALSE>>=
n <- 1000
train = matrix(data = NA, nrow = n, ncol = 28*28 )

for (i in 1:n)
  {
train[i,] <- as.vector(TRAIN[,,i])
  }
train_labels <- Train_labels[1:n]
@

<<echo=FALSE>>=
m <- 500
test = matrix(data = NA, nrow = m, ncol = 28*28 )

for (i in 1:m)
{
test[i,] <- as.vector(TEST[,,i])
}

test_labels <- Test_labels[1:m]
@

Теперь воспользуемся функцией \code|knn|, чтобы классифицировать объекты тестовой выборки, и запишем результаты в~вектор \code|results|. Возьмём, например, $k=10$.
Преимуществом функции является высокая скорость подсчёта предсказаний.

<<error=FALSE, warning=FALSE>>=
library(FNN)
results <- (0:9)[knn(train, test, train_labels, k = 10,
                     algorithm = "cover_tree")]
@

Посмотрим на долю ошибок, которые мы допустили при классификации.
Она достаточно высока, но это можно объяснить тем, что мы использовали слишком маленькое для такого алгоритма число наблюдений.

<<>>=
errors <- (sum(results != test_labels))/m
errors
@

\subsection{Пакет \pkg{RWeka}}

Пакет \pkg{RWeka} (\cite{RWekapackage14}) позволяет использовать из \proglang{R} все возможности среды для машинного обучения \proglang{Weka}. Среда \proglang{Weka}, \url{http://www.cs.waikato.ac.nz/ml/weka}, содержит целый набор алгоритмов машинного обучения, среди которых есть и~нужный нам.
В~\proglang{Weka} замечательно то, что алгоритм сам подбирает оптимальное в нашем случае значение $k$ из заданного диапазона, а~потом оценивает полученный классификатор с~помощью пятикратной кросс"=валидации.
Это значит, что он поделит выборку на пять кусочков и~по очереди будет использовать их в~качестве тестовых, чтобы надёжнее оценить классификацию, исключив возможность того, что хорошие или плохие предсказания получились под влиянием какого-либо конкретного набора данных.

Устанавливать отдельно \proglang{Weka} не нужно, необходимые файлы пакет \pkg{RWeka} установит сам. Кстати, \proglang{Weka} разработана в Новой Зеландии, на родине \proglang{R}, и вообще, \proglang{Weka} --- это птичка:

\begin{figure}[hbtp]
\caption{Новозеландская курица \proglang{Weka}}
\centering
\includegraphics[scale=0.2]{SI_Weka.jpg}
\end{figure}

Можно послушать, как она поёт, \url{http://www.cs.waikato.ac.nz/ml/weka/sounds/weka-long.au}.


Однако при загрузке пакета нужно убедиться, что на компьютере установлена среда Java, и~отдавать себе отчёт, что поиск подходящих параметров и~пятикратная оценка классификатора будет требовать значительного времени.

Используем функцию \code|IBk|.
Для этого добавим слева к~нашей матрице \code|train| столбец с~ответами и~преобразуем матрицу в~объект \code|data.frame|.
Выражение \code|K = 10| указывает на то, что алгоритм будет перебирать все значения от~1 до~10.
Функция \code|evaluate_weka_classifier| оценивает качество результатов и~показывает, какие значения были классифицированы правильно в~результате пятикратного разбиения выборки на кусочки и~последующего усреднения, а~какие "--- нет.

Обратите внимание на \ENGs{Confusion matrix}, которая показывает, с~какими именно классами возникали ошибки: можно заметить, что пятёрки и~восьмёрки классифицировались как всё что угодно.

<<warning=FALSE>>=
library(RWeka)

train_weka <- data.frame(train_labels, train)
train_weka[,1] <- as.factor(train_weka[,1])
classifier <- IBk(train_labels~., data = train_weka,
                  control = Weka_control(K = 10, X=TRUE))
classifier
evaluate_Weka_classifier(classifier, numFolds = 5)
@

Теперь предскажем значения для тестовой выборки с~помощью функции \code|predict| и~посмотрим на долю ошибок.

<<warning=FALSE>>=
test_weka = data.frame(test)
names(test_weka) = names(train_weka)[-1]

predictions <- predict(classifier, newdata = test_weka)

errors <- (sum(predictions != test_labels))/m
errors
@

\section{Заключение}

Таким образом, мы рассмотрели интересную задачу и~отличную, заботливо собранную базу данных, применили к~ним один из самых первых и~простых алгоритмов машинного обучения, который, однако, часто показывает очень хорошие результаты.
Но это был один только пример, а~в~качестве инструментов в~данном случае могут быть использованы разнообразнейшие алгоритмы.
Кроме того, можно подумать над тем, как преобразовать сами изображения для улучшения качества классификации. Удачи!


\printbibliography


\end{document}
