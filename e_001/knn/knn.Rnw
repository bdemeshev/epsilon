\documentclass[final,pdftex]{../../template/epsilonj}

\RequirePackage{graphicx}
\usepackage{verbatim}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\addbibresource{../../template/epsilon.bib}

\begin{document}
\setcounter{page}{6}
	
	\begin{frontmatter}
		\title{Распознавание рукописных цифр}
		\runtitle{Распознавание рукописных цифр}
		
		\begin{aug}
			\author{\imya{Саша} \fam{Кузнецова}}%
			
			\runauthor{Саша~Кузнецова}
			
			\address{НИУ ВШЭ, Москва.}
		\end{aug}
		
		\begin{abstract}
		Задача распознавания рукописного текста "--- одна из классических задач машинного обучения, к~решению которой применялось такое количество алгоритмов, что она успешно может быть использована в~качестве учебной. 
		
		В~этой заметке мы будем учиться распознавать написанные от руки цифры.
		\end{abstract}
		
		\begin{keyword}
			\kwd{k ближайших соседей}
			\kwd{R}
			\kwd{машинное обучение}
			\kwd{распознавание образов}
		\end{keyword}
		
	\end{frontmatter}
	
		
Мы обратимся к~одному из наиболее известных хранилищ, в~котором собраны обработанные изображения цифр, разберём, как эти данные оттуда извлекать, а~затем применим к~ним алгоритм $k$ ближайших соседей, используя \proglang{R}.
	
	\section{База данных MNIST}
	
Источником данных нам послужит база «MNIST» (\cite{mnistdigits}), \url{http://yann.lecun.com/exdb/mnist/}, в~которой хранятся 70\,000 изображений цифр, написанных несколькими сотнями разных людей. 
Каждая картинка в~этой базе обработана так, чтобы поместиться в~квадратик размером $28\times28$ пикселей. 
Каждый пиксель представлен числом от~0 до~255, где 0~соответствует белому цвету, а~255 "--- чёрному. 

Все изображения поделены на две части: 60\,000 относятся к~учебной выборке, а~10\,000 "--- к~тестовой. 
По учебной выборке наш алгоритм будет настраивать свои параметры, а~по тестовой мы будем оценивать качество классификации.
Такое разбиение нужно для того, чтобы убедиться, что алгоритм не переобучился, то есть не настроен исключительно на учебные примеры и~способен правильно классифицировать новые для него изображения.

Каждое из изображений в~нашей задаче должно быть отнесено к~одному из десяти классов "--- это цифры от~0 до~9. 
Для элементов обучающей выборки известно, к~какому классу они принадлежат, поэтому настройка параметров классифицирующего алгоритма относится к~области \textit{обучения с~учителем}. 
Это значит, что процесс обучения использует известные ответы и~стремится за счёт выбора параметров сделать предсказания алгоритма максимально близкими к~правильным (кстати, таким образом мы обучали все наши алгоритмы в~курсе эконометрики).

Как говорится, «наивные студенты думали, что .csv"=файлы на деревьях, как булки, растут», но всё оказалось совсем не так. 
Для того чтобы прочитать IDX-файл, в~котором хранятся данные с~изображениями цифр в~базе «MNIST», мне потребовались Google и~кандидат физико"=математических наук. 
Проблема заключается в~том, что данные хранятся в~этой базе в~бинарном виде, и~их не открыть в~привычных нам приложениях.
Вот как это можно сделать в~\proglang{R}.

\par\medskip
(1) Скачав данные с~сайта, открываем файл с~учебной выборкой на чтение командой \code|file|:

<<>>=
to.read <- file("train-images-idx3-ubyte", "rb")
@

\par\medskip (2) Данные устроены таким образом, что в~самом начале мы читаем заголовок из четырёх чисел. Эти первые четыре числа содержат информацию о~размерах выборки, упомянутых выше. Считываем их из полученного объекта \code|to.read| с~помощью функции для чтения бинарных данных \code|readBin|.

<<>>=
readBin(to.read, integer(), n = 4, endian="big")
@

\par\medskip (3) Далее для каждой картинки следуют $28\times28$ байт, содержащие информацию о~цвете каждого пикселя (числа от~0 до 255) и~записанные из изображения построчно. 
Получаем большой массив \code|TRAIN| из 60\,000 таблиц размером $28\times28$: берём первые 28 чисел и~кладём их в~первый столбец первой таблицы, продолжаем, пока не дойдём до 28-го столбца, затем приступаем к~следующей таблице "--- и~так до конца. 

Нужно обратить внимание, что изначально данные были разложены в~строку, но мы только что разложили их по столбцам, для того чтобы позже было удобнее выводить рисунок.

<<warning=FALSE>>=
TRAIN <- array(data = NA, dim = c(28,28,60000))

for(i in 1:60000)
  {
TRAIN[,,i] <- matrix(readBin(to.read,integer(), size = 1,
    n = 28*28, signed = FALSE, endian = "big"), 28, 28);
  }
close(to.read)
@

\par\medskip (4) Перед тем как начинать работать с~данными и~оценивать по ним какие-либо модели, обычно бывает полезно взглянуть на них и~построить описательные статистики. В~данном случае для этой цели может послужить сама картинка.

<<>>=
layout(matrix(c(1:36), 6, 6, byrow = TRUE),
   widths = lcm(rep(2.5,36)), heights = lcm(rep(2.5,36)))
par(mar=c(0,0,0,0))

for(i in 1:36)
  {
image(TRAIN[,28:1,i])
  }
@

Нарисуем цифры по первым 36~таблицам из массива \code|TRAIN|. 
Нужно обратить внимание, что, нарисуй мы сейчас всё как есть, мы бы получили перевёрнутые цифры (это можно проверить), потому что функция \code|image| будет соотносить первый столбец с~нулевой ординатой на картинке, но мы помним, что первый столбец соответствовал верхней строке рисунка. 
Для того чтобы избежать этой проблемы, будем рисовать столбцы в~обратном порядке, от 28-го к~1-му: \code|TRAIN[,28:1,]|.

\par\medskip (5) Описанную выше процедуру повторяем, чтобы считать метки классов для учебной выборки. 
Заголовок в~данном случае состоит из двух чисел, а~данные должны восприниматься \proglang{R}'ом не как числа, а~как категории: этого можно добиться с~помощью команды \code|as.factor|.

<<>>=
to.read <- file("train-labels-idx1-ubyte", "rb")
header <- readBin(to.read, integer(), n=2, endian="big")
Train_labels <- readBin(to.read, integer(), size = 1,
            n = 60000, signed = FALSE, endian="big")
Train_labels <- as.factor(Train_labels)
close(to.read)
@

\par\medskip
Итак, на данном этапе мы являемся обладателями учебной выборки (но умеем читать и~многое другое) и~великолепной картинки, что означает, что пора бы с~ними что-нибудь да~сделать. 

\section{Алгоритм $k$ ближайших соседей}

Одним из простейших алгоритмов классификации является метод $k$ ближайших соседей (\ENGs{$k$ Nearest Neighbours}, kNN). 
Основной идеей данного алгоритма является то, что объект, как правило, находится в~окружении объектов своего же класса. 
Таким образом, чтобы классифицировать новый объект, мы должны посмотреть на $k$ ближайших к~нему объектов из учебной выборки и~отнести его к~тому классу, который среди них чаще встретился.

Первый вопрос, который перед нами встаёт, "--- это выбор функции расстояния между объектами выборки.
В~простом случае, когда объекты представлены в~виде числовых векторов, пользуются простой евклидовой метрикой, то есть $\rho(a, b) = \sqrt{\sum\limits_{k = 1}^m(a_k-b_k)^2}$.

Второй проблемой является выбор значения $k$: сколько объектов нам нужно посмотреть, чтобы наиболее точно классифицировать наш? 
Единого правила для того, чтобы выбрать $k$, не существует, однако нужно учитывать, что при слишком маленьких $k$ алгоритм будет неустойчивым, а~при слишком больших начнёт подстраиваться к~шуму и~терять обобщающую способность. 
Конкретное значение $k$ можно определить опытным путём: попробовать диапазон значений и~посмотреть, какое подходит лучше.

\subsection{Существующие реализации}

Для реализации алгоритма $k$ ближайших соседей в~\proglang{R} есть как минимум три пакета: \pkg{kknn}, \pkg{FNN} и~\pkg{RWeka}, причём в~двух последних есть ещё и~готовые базы данных.

Пакет \pkg{kknn} реализует алгоритм kNN с~весами, где голоса «соседей» не равнозначны, а~входят в~общую сумму с~определёнными весами. Мы его реализовывать не будем.

Воспользуемся функцией \code|install.packages(c("FNN", "RWeka"))|, чтобы загрузить пакеты.

\subsection{Пакет \pkg{FNN}}

Воспользуемся теперь пакетом \pkg{FNN} (\cite{RFNNpackage14}), предварительно проделав с~данными для тестовой выборки то же, что и~с~учебными, в~итоге получив два больших массива, \code|TRAIN| и~\code|TEST|, а~также два вектора ответов, \code|Trainlabels| и~\code|Testlabels|. 

<<echo=FALSE>>=
to.read <- file("t10k-images-idx3-ubyte", "rb")
header <- readBin(to.read, integer(), n=4, endian="big")

TEST <- array(data = NA, dim = c(28,28,10000))

for(i in 1:10000)
  {
TEST[,,i] <- matrix(readBin(to.read,integer(), size=1, n=28*28,signed = FALSE, endian="big"),28,28);
  }
close(to.read)
to.read <- file("t10k-labels-idx1-ubyte", "rb")
header <- readBin(to.read, integer(), n=2, endian="big")

Test_labels <- readBin(to.read, integer(), size = 1, n=10000, signed = FALSE, endian="big")
close(to.read)
Test_labels <- as.factor(Test_labels)
@

Функция \code|knn| пакета \pkg{FNN} в~качестве аргументов принимает матрицы учебных и~тестовых данных, а~также вектор ответов для тренировочной выборки, а~на выходе отдаёт предсказанную классификацию для тестовой выборки. 
Эта функция сразу и~обучается, и~предсказывает "--- очень удобно.

Сейчас наши данные выглядят не так, как нужно этой функции, поэтому преобразуем их в~матрицу, содержащую значения цвета всех пикселей, размещённых в~одну строку для каждой картинки. 
Воспользуемся функцией \code|as.vector|, которая будет по очереди брать столбцы из матрицы и~выкладывать их в~одну строку. 
Кроме того, для примера мы будем использовать только первую тысячу наблюдений из учебной выборки и~пятьсот из тестовой. 
Если читатель желает получить более точный классификатор, то в~этом месте следует использовать все 60\,000 и~10\,000 соответственно.

<<warning=FALSE>>=
n <- 1000
train = matrix(data = NA, nrow = n, ncol = 28*28 )

for (i in 1:n)
  {
train[i,] <- as.vector(TRAIN[,,i])
  }
train_labels <- Train_labels[1:n]
@

<<echo=FALSE>>=
m <- 500 
test = matrix(data = NA, nrow = m, ncol = 28*28 )

for (i in 1:m)
{
test[i,] <- as.vector(TEST[,,i])
}

test_labels <- Test_labels[1:m]
@

Теперь воспользуемся функцией \code|knn|, чтобы классифицировать объекты тестовой выборки, и запишем результаты в~вектор \code|results|. Возьмём, например, $k=10$. 
Преимуществом функции является высокая скорость подсчёта предсказаний.

<<error=FALSE, warning=FALSE>>=
library(FNN)
results <- (0:9)[knn(train, test, train_labels, k = 10,
                     algorithm = "cover_tree")]
@

Посмотрим на долю ошибок, которые мы допустили при классификации. 
Она достаточно высока, но это можно объяснить тем, что мы использовали слишком маленькое для такого алгоритма число наблюдений.

<<>>=
errors <- (sum(results != test_labels))/m
errors
@

\subsection{Пакет \pkg{RWeka}} 

Пакет \pkg{RWeka} (\cite{RWekapackage14}) позволяет использовать из \proglang{R} все возможности среды для машинного обучения \proglang{Weka}. Среда \proglang{Weka}, \url{http://www.cs.waikato.ac.nz/ml/weka}, содержит целый набор алгоритмов машинного обучения, среди которых есть и~нужный нам. 
В~\proglang{Weka} замечательно то, что алгоритм сам подбирает оптимальное в нашем случае значение $k$ из заданного диапазона, а~потом оценивает полученный классификатор с~помощью пятикратной кросс"=валидации. 
Это значит, что он поделит выборку на пять кусочков и~по очереди будет использовать их в~качестве тестовых, чтобы надёжнее оценить классификацию, исключив возможность того, что хорошие или плохие предсказания получились под влиянием какого-либо конкретного набора данных. 

Устанавливать отдельно \proglang{Weka} не нужно, необходимые файлы пакет \pkg{RWeka} установит сам. Кстати, \proglang{Weka} разработана в Новой Зеландии, на родине \proglang{R}, и вообще, \proglang{Weka} --- это птичка:

\begin{figure}[hbtp]
\caption{Новозеландская курица \proglang{Weka}}
\centering
\includegraphics[scale=0.2]{SI_Weka.jpg}
\end{figure}

Можно послушать, как она поёт, \url{http://www.cs.waikato.ac.nz/ml/weka/sounds/weka-long.au}.


Однако при загрузке пакета нужно убедиться, что на компьютере установлена среда Java, и~отдавать себе отчёт, что поиск подходящих параметров и~пятикратная оценка классификатора будет требовать значительного времени.

Используем функцию \code|IBk|. 
Для этого добавим слева к~нашей матрице \code|train| столбец с~ответами и~преобразуем матрицу в~объект \code|data.frame|. 
Выражение \code|K = 10| указывает на то, что алгоритм будет перебирать все значения от~1 до~10. 
Функция \code|evaluate_weka_classifier| оценивает качество результатов и~показывает, какие значения были классифицированы правильно в~результате пятикратного разбиения выборки на кусочки и~последующего усреднения, а~какие "--- нет. 

Обратите внимание на \ENGs{Confusion matrix}, которая показывает, с~какими именно классами возникали ошибки: можно заметить, что пятёрки и~восьмёрки классифицировались как всё что угодно.

<<warning=FALSE>>=
library(RWeka)

train_weka <- data.frame(train_labels, train)
train_weka[,1] <- as.factor(train_weka[,1])
classifier <- IBk(train_labels~., data = train_weka,
                  control = Weka_control(K = 10, X=TRUE))
classifier
evaluate_Weka_classifier(classifier, numFolds = 5)
@

Теперь предскажем значения для тестовой выборки с~помощью функции \code|predict| и~посмотрим на долю ошибок.

<<warning=FALSE>>=
test_weka = data.frame(test)
names(test_weka) = names(train_weka)[-1]

predictions <- predict(classifier, newdata = test_weka)

errors <- (sum(predictions != test_labels))/m
errors
@

\section{Заключение}

Таким образом, мы рассмотрели интересную задачу и~отличную, заботливо собранную базу данных, применили к~ним один из самых первых и~простых алгоритмов машинного обучения, который, однако, часто показывает очень хорошие результаты. 
Но это был один только пример, а~в~качестве инструментов в~данном случае могут быть использованы разнообразнейшие алгоритмы. 
Кроме того, можно подумать над тем, как преобразовать сами изображения для улучшения качества классификации. Удачи!


\printbibliography


\end{document}