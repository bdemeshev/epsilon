\documentclass[10pt]{article}

\usepackage{epsilonj}

\usepackage{tikz}
\usetikzlibrary{calc,intersections,decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
% этот код считает отмечает углы в tikz
\newcommand\markangle[6]{% origin X Y radius radiusmark mark
  % fill red circle
  \begin{scope}
    \path[clip] (#1) -- (#2) -- (#3);
    \fill[color=red,fill opacity=0.5,draw=red,name path=circle]
    (#1) circle (#4);
  \end{scope}
  % middle calculation
  \path[name path=line one] (#1) -- (#2);
  \path[name path=line two] (#1) -- (#3);
  \path[%
  name intersections={of=line one and circle, by={inter one}},
  name intersections={of=line two and circle, by={inter two}}
  ] (inter one) -- (inter two) coordinate[pos=.5] (middle);
  % put mark
  \node at ($(#1)!#5!(middle)$) {#6};
}

\RequirePackage{graphicx}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\RR}{\mathbb{R}}
\def \hb{\hat{\beta}}
\DeclareMathOperator{\pCorr}{pCorr}


\begin{document}

\TITLE{Корреляция: простая, частная и условная}
\SHORTTITLE{Корреляция: простая, частная и условная}

\AUTHOR{Борис Демешев}{НИУ ВШЭ, Москва.}
\SHORTAUTHOR{Борис Демешев}

\DoFirstPageTechnicalStuff


\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Определение}


\begin{abstract}
Корреляция "--- это способ описать силу линейной зависимости между двумя случайными величинами одним числом. Каков геометрический смысл корреляции? Что такое частная корреляция? Как связаны частная и условная корреляция?
\end{abstract}

\begin{keyword}
корреляция, частная корреляция, условная корреляция, косинус, проекция
\end{keyword}


\section{Сколько вешать в граммах?}

% Люди часто хотят найти простое решение сложной задачи.

Почему при болезни мы измеряем температуру тела? По всей видимости важны два факта:

\begin{enumerate}
\item Измерить температуру очень удобно
\item Это измерение несёт в себе информацию о здоровье
\end{enumerate}

Сложное описание состояния здоровья сводится измерением температуры к одной цифре. Естественно, куча информации теряется в этой цифре и бессмысленно лечить человека, руководствуясь только температурой его тела. Температура $40^{\circ}$ говорит, что что-то не так, но что --- непонятно. А температура $36.6^{\circ}$ ещё не говорит о том, что у человека идеальное здоровье. Однако процедура очень проста и в некоторых ситуациях, например, при обыкновенной простуде её достаточно для принятия решения о приёме жаропонижающего.

Если бы для измерения температуры нужно было специальное устройство размером с половину комнаты, никто бы дома её не мерял. Простота измерения очень важна!

Подобном образом дела обстоят и с описанием зависимости между случайными величинами. Зависимость между случайными величинами полностью описывается их совместной функцией распределения, $F(x,y)=\P(X\leq x, Y\leq y)$. Это слишком сложно! Вместо сложной функции распределения мы хотим получить одно число. Некую «силу зависимости». Назовём это мифическое число $Dep(X,Y)$.

Чего бы нам хотелось от этого числа?
\begin{enumerate}
\item Посчитать это число очень удобно
\item Это число несёт в себе информацию о зависимости
\end{enumerate}

В случае двух случайных величин наиболее популярными мерами зависимости являются ковариация, корреляция и коэффициент в парной регрессии.

\section{Ковариация}

В каком смысле «удобно» считать?

Очень часто возникают суммы случайных величин, поэтому было бы здорово, чтобы у суммы легко считалась наша характеристика $Dep(X+Z,Y)$. Проще всего было бы, если бы:

\[
Dep(X+Z,Y)=Dep(X,Y)+Dep(Z,Y)
\]

И, конечно, мы ждём, что у независимых случайных величин нулевая сила зависимости, $Dep(X,Y)=0$, а ненулевая сила зависимости, $Dep(X,Y)\neq 0$, была бы возможна только у зависимых случайных величин.

Этим двум требуемым свойствам, простота подсчёта и информация о зависимости, отвечает ковариация.

\begin{definition}
Ковариация величин $X$ и $Y$ измеряет ...
\[
\Cov(X,Y)=\E\left[  (X-\E(X)) (Y-\E(Y))   \right] = \E(XY)-\E(X)\E(Y)
\]
\end{definition}

здесь рассказать про прямоугольники и площадь с плюсом/минусом?

\section{Коэффициент в парной регрессии}


...

Недостаток коэффициента парной регрессии в его несимметричности. В регрессиях $y$ на $x$ и $x$ на $y$ коэффициенты не только не обязаны совпадать, но даже в произведении чаще всего не дают единицу.

От несимметричности коэффициент в парной регрессии можно избавить отмасштабировав $y$ и $x$ на их стандартные отклонения. При этом мы получим стандартизованный коэффициент парной регрессии.

...


Естественно, стандартизованный коэффициент связан с обычным соотношением:
\[
\beta^{st}=\beta \cdot \frac{\sigma_X}{\sigma_Y}
\]

По смыслу стандартизованный коэффициент показывает на сколько своих стандартных отклонений в среднем растёт величина $Y$ при росте величины $X$ на одно своё стандартное отклонение. И именно этот стандартизированный коэффициент парной регрессии и называется корреляцией.



\section{Корреляция по-русски}

Обычно в учебниках даётся такое определение корреляции

\begin{equation}
\label{def_corr}
\Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}.
\end{equation}

Естественно, возникает вопрос: «С какого перепугу? Почему это мы делим ковариацию на что-то там?»

Мы дадим ещё раз дадим определение корреляции словами:

\begin{definition}
Корреляция между случайными величинами $X$ и $Y$ показывает на сколько своих стандартных отклонений в среднем растёт случайная величина $Y$ при росте случайной величины $X$ на одно своё стандартное отклонение.
\end{definition}

А теперь из этого словесного определения мы получим формулу \ref{def_corr}. Разложим величину $Y$ на два слагаемых. Первое слагаемое вбирает в себя всю ту часть $Y$, которая линейно зависит от $X$, а второе "--- всё оставшееся:

\[
\frac{Y}{\sigma_Y}=\rho \cdot \frac{X}{\sigma_X} + \varepsilon
\]

В этой формуле видно, что с ростом $X$ на одно стандартное отклонений $\sigma_X$ правая часть изменится в среднем на $\rho$, и, следовательно, величина $Y$ в среднем изменится на $\rho \cdot \sigma_Y$.

%\[
%Y=\beta \cdot X + \varepsilon
%\]
%
%Здесь $\beta$ показывает на сколько единиц в среднем растёт $Y$ при росте $X$ на одну единицу.

Естественно, мы хотим, чтобы с ростом $X$ величина $\varepsilon$ в среднем не менялась, то есть хотим нулевую «силу зависимости» между ними, $\Cov(X, \varepsilon)=0$.


\[
\Cov\left(X, \frac{Y}{\sigma_Y} - \rho \cdot \frac{X}{\sigma_X} \right) = 0
\]

По свойствам ковариации получаем

\[
\Cov(X,Y)/\sigma_Y- \rho \Cov(X,X)/\sigma_X=0
\]

И, тадам, выражаем корреляцию, $\rho$:

\[
\rho = \frac{\Cov(X,Y)/\sigma_Y}{\Cov(X,X)/\sigma_X} = \frac{\Cov(X,Y)}{\sigma_X \sigma_Y}
\]

Отметим асимметричность исходного разложения: эпсилон прибавляется в правой части уравнения к величине $X$. Несмотря на эту асимметричность результирующая формула для корреляции получается симметричной. Из этого следует, что ровно такой же результат получится, если начать с разложения:
\[
\frac{X}{\sigma_X}=\rho \cdot \frac{Y}{\sigma_Y} + \varepsilon
\]

Из определения неочевидно, что корреляция лежит в пределах от $-1$ до $1$. ...


Стоит обратить внимание на немного контр-интуитивный факт. Если бы зависимость между $X$ и $Y$ была бы жесткой детерминистической, и с ростом $X$ на единицу величина $Y$ росла бы на $\Delta$, то с ростом $Y$ на единицу величина $X$ росла бы на $1/\Delta$. Для случайных величин обращения не происходит. Если с ростом $X$ на одно своё стандартное отклонение величина $Y$ в среднем растёт на $\rho$ своих стандартных отклонений, то и с ростом $Y$ на одно своё стандартное отклонение величина $X$ в среднем растёт на $\rho$ своих стандартных отклонений.





\section{Геометрический смысл корреляции}

Давайте рисовать случаные величины векторами-стрелочками! Не в том смысле, что у стрелочки случайное направление или длина, а в том смысле, что направление и длина стрелочки описывают характеристики этой случайной величины.

Любую геометрию можно задать, задав скалярное произведение. Действительно, если мы умеем считать скалярное произведение двух любых векторов, $<\vec{a},\vec{b}>$, то длина вектора считается ровно как в 9-м классе:

\[
|\vec{a}|=\sqrt{<\vec{a},\vec{a}>}
\]

И также любой девятикласник помнит, что косинус угла между векторами считается как

\[
\cos(\vec{a},\vec{b})=\frac{<\vec{a},\vec{b}>}{|\vec{a}||\vec{b}|}
\]

Мы определим скалярное произведение двух случайных величин как их ковариацию:

\[
<X,Y>=\Cov(X,Y)
\]

При таком подходе длиной случайной величины окажется стандартное отклонение:

\[
\sqrt{\Cov(X,X)}=\sqrt{\Var(X)}=\sigma_X
\]

А корреляция окажется косинусом угла между случайными величинами:
\[
\cos \phi =\cos(X,Y)=\frac{\Cov(X,Y)}{\sigma_X\sigma_Y}=\Corr(X,Y)
\]


\begin{center}
\begin{tikzpicture}
\path (0,0) coordinate (origin);
\path (origin) ++(1,2) coordinate (Y);
\path (origin) ++(3,0) coordinate (X);
\draw[-{Latex[length=3mm]},thick] (origin) -- (Y);
\draw[-{Latex[length=3mm]},thick] (origin) -- (X);
\node [above] at (X) {$X$};
\node [right] at (Y) {$Y$};
\markangle{origin}{X}{Y}{6mm}{4mm}{$\phi$}
\end{tikzpicture}
\end{center}

Значит в нашей геометрии  длина стрелочки "--- стандартное отклонение случайной величины, а косинус угла между двумя стрелочками "--- это корреляция двух случайных величин. Дисперсия, следовательно, это квадрат длины случайной величины. Перпендикулярными случайными величинами будут те, косинус угла между которыми равен нулю, то есть некоррелированные.

Например, сформулируем в данной геометрии теорему Пифагора. Если случайные величины $X$ и $Y$ перпендикулярны (корреляция или ковариация равны нулю), то дисперсия их суммы (квадрат длины гипотенузы) равен сумме их дисперсий (сумму квадратов длин катетов):

\[
\Var(X+Y)=\sigma^2_{X+Y}=\sigma^2_X+\sigma^2_Y=\Var(X)+\Var(Y)
\]

\begin{center}
\begin{tikzpicture}
\path (0,0) coordinate (origin);
\path (origin) ++(4,2) coordinate (Y);
\path (origin) ++(4,0) coordinate (X);
\draw[-{Latex[length=3mm]},thick] (origin) -- (Y);
\draw[-{Latex[length=3mm]},thick] (origin) -- (X);
\draw[-{Latex[length=3mm]},thick] (X) -- (Y);
\node [yshift=10pt,xshift=-15pt] at (X) {$X$};
\node [yshift=-15pt,xshift=15pt] at (Y) {$Y$};
\node [yshift=5pt,xshift=-30pt] at (Y) {$X+Y$};
\draw [decorate,decoration={brace,amplitude=10pt}] (X) -- (origin) node [black,midway,yshift=-15pt] {\footnotesize $\sigma_X$};
\draw [decorate,decoration={brace,amplitude=10pt}] (Y) -- (X) node [black,midway,xshift=20pt] {\footnotesize $\sigma_{Y}$};
\draw [decorate,decoration={brace,amplitude=10pt}] (origin) -- (Y) node [black,midway,yshift=15pt,xshift=-10pt] {\footnotesize $\sigma_{X+Y}$};
\end{tikzpicture}
\end{center}

Введение геометрии позволяет говорить о проекции. Например, можно спроецировать случайную величину $Y$ на множество случайных величин пропорциональных величине $X$, $\{cX | c\in \RR \}$. Если на обычной плоскости спроецировать вектор $\vec{a}$ на прямую, порожденную вектором $\vec{b}$, то получится $\cos(\vec{a},\vec{b})\cdot \vec{b}$. По аналогии, если спроецировать случайную величину $Y$ на множество $\{cX | c\in \RR \}$, то получится $\Corr(X,Y)\cdot X$. Другими словами, среди случайных величин пропорциональных $X$ величина  $\hat{Y}=\Corr(X,Y)\cdot X$ --- самая похожая на величину $Y$.

Понятие проекции позволяет интепретировать квадрат корреляции. Квадрат косинуса равен отношению квадрата длины прилежащего катета $\Var(\hat{Y})$ к квадрату гипотенузы $\Var(Y)$.

(картинка)

Следовательно, $\Corr(X,Y)^2=\frac{\Var(\hat{Y})}{\Var(Y)}$, то есть квадрат корреляции показывает долю дисперсии $Y$, которую можно объяснить с помощью величин пропорциональных $X$.

\section{Корреляция и независимость}

\begin{theorem}
Случайные величины $X$ и $Y$ независимы тогда и только тогда, когда некоррелированы любые функции $f(X)$ и $g(Y)$.
\end{theorem}

Другими словами для независимости $X$ и $Y$ необходима некоррелированность пар $X$ и $Y$, $X^2$ и $\cos(Y)$, $\exp(X)$ и $1/Y$, и так далее. Из этого следует, что некоррелированность $X$ и $Y$ является необходимым, но недостаточным условием для независимости.

Можно выделить три «степени» независимости случайных величин $X$ и $Y$:

\begin{tabular}{rl}
\toprule
Некоррелированность $Y$ и $X$ & $\Corr(X,Y)=0$ \\
$\E(Y|X)=\E(Y)$ & $\Corr(f(X),Y)=0$ для всех $f()$ \\
Независимость $Y$ и $X$ & $\Corr(f(X),g(Y))=0$ для всех $f()$ и $g()$ \\
\bottomrule
\end{tabular}



Многие ошибочно считают, что если величина $X$ имеет нормальное распределение $N(\mu_X, \sigma^2_X)$ и величина $Y$ имеет нормальное распределение $N(\mu_Y, \sigma^2_Y)$, и $X$ и $Y$ некоррелированы, то они независимы. Это неверно.

Контрпример. Случайная величина $X$ имеет стандартное нормальное распределение $N(0;1)$, случайная величина $Z$ независима от $X$ и равновероятно принимает значения $-1$ и $+1$. Определим величину $Y$ как их произведение, $Y=XZ$.

В этом примере величины $X$ и $Y$ зависимы, так как $|X|=|Y|$. Однако $Y$ распределена нормально стандартно и $\Corr(X,Y)=0$.

Правильная теорема звучит так:

\begin{theorem}
Если некоррелированные случайные величины $X$ и $Y$ имеют совместное нормальное распределение, то $X$ и $Y$ независимы.
\end{theorem}

Попутно упомянем ещё одно неожиданное свойство предъявленного контрпримера. Если случайные величины нормальны по отдельности, то вполне возможно, что их сумма ненормальна. Для пары величин, имеющих совместное нормальное распределение, это невозможно.



\section{Более двух случайных величин}

Если случайных оказывается больше двух, то для описания зависимости с помощью одного числа оказывается очень много вариантов. Среди популярных можно назвать:
\begin{enumerate}
\item коэффициент бета в множественной регрессии
\item стандартизированный коэффициент множественной регрессии
\item частная корреляция
\item условная корреляция
\item каноническая корреляция
\end{enumerate}


\section{Стандартизованный коэффициент регрессии}

Для нахождения стандартизованного коэффициента множественной регрессии используется разложение
\[
\frac{Y}{\sigma_Y}=\rho_{XY|Z} \cdot \frac{X}{\sigma_X} + \rho_{YZ|X} \frac{Z}{\sigma_Z} + \varepsilon
\]

Два подхода к определению стандартизованного коэффициента регрессии эквивалентны в силу теоремы Фриша-Ву-Ловелла (Frisch–Waugh–Lovell). Обычно эта теорема формулируется применительно к регрессии, а здесь мы приведём её вариат для случайных величин.

\begin{theorem}
Если имеют место разложения:
\[
Y=a_1 Z_1 + a_2 Z_2 + \ldots + a_n Z_n + \tilde{Y}, \text{ где }\; \tilde{Y}\bot Z_1, Z_2, \ldots, Z_n
\]
и
\[
X=b_1 Z_1 + b_2 Z_2 + \ldots + b_n Z_n  + \tilde{X}, \text{ где }\; \tilde{X}\bot Z_1, Z_2, \ldots, Z_n
\]
То в разложениях
\[
\tilde{Y}=d \tilde{X} + \varepsilon, \text{ где }\; \varepsilon \bot \tilde{X}
\]
и
\[
Y=c_1 Z_1 + c_2 Z_2 + \ldots + c_n Z_n  + dX + u, \text{ где }\; u \bot Z_1, Z_2, \ldots, Z_n, X
\]
коэффициенты при $\tilde{X}$ и $X$ совпадают.
\end{theorem}


\section{Частная корреляция}

\begin{definition}
Частная корреляция между величинами $X$ и $Y$ при фиксированной величине $Z$ показывает на сколько своих стандартных отклонений в среднем вырастет величина $Y$, очищенная от связи с $Z$, при росте величины $X$, очищенной от связи с $Z$, на одно своё стандартное отклонение.
\end{definition}


Подход к подсчёту частной корреляции следующий:
\begin{enumerate}
\item Спроецируем $X$ на множество величин, некоррелированных с $Z$. Получим $\tilde{X}$.
\item Спроецируем $Y$ на множество величин, некоррелированных с $Z$. Получим $\tilde{Y}$.
\item Частная корреляция между $X$ и $Y$ при фиксированной $Z$ "--- это обычная корреляция между $\tilde{X}$ и $\tilde{Y}$.
\end{enumerate}


(картинка ...)

\section{Частная дисперсия и ковариация}

Определения частной корреляции можно сформулировать с помощью частной ковариации и частной дисперсии.

\[
pCorr(X, Y|Z) = \frac{pCov(X,Y|Z)}{\sqrt{pVar(X|Z)pVar(Y|Z)}}
\]

Частная дисперсия величины $X$ очищенной от $Z$ определяется как

\[
pVar(X|Z) = \Var(X-aZ), \text{ где  константа } a \text{ такова, что} \Cov(X-cZ, Z) = 0
\]

Частная ковариация

\[
pCov(X,Y|Z) = \Cov(X-aZ, Y- bZ ), \text{ где  константы } a \text{ и } b \text{ такова, что} \Cov(X-aZ, Z) = \Cov(Y-bZ, Z) = 0,
\]


\section{Условная корреляция}

\begin{definition}
Условная корреляция между величинами $X$ и $Y$ при известном значении величины $Z$ показывает на сколько своих стандартных отклонений $\sigma_Y$ в среднем вырастет $Y$ при росте величины $X$ на одно своё стандартное отклонение $\sigma_X$ при заданном значении величины $Z$.
\end{definition}


Следует подчеркнуть одно существенное отличие условной корреляции от обычной и частной. Обычная и частная корреляция являеются константами. Условная корреляция $\Corr(X,Y|Z)$ является функцией от $Z$. Величина $Z$ является случайной, поэтому и условная корреляция $\Corr(X,Y|Z)$ является случайной величиной.

Здесь регрессионное определение????

Чуть более формальное определение:

\begin{definition}
\[
\Corr(X,Y|Z) = \frac{\Cov(X,Y|Z)}{\sqrt{\Var(X|Z)\Var(Y|Z)}},
\]
\end{definition}

где $\Cov(X,Y|Z)=\E(XY|Z)-\E(X|Z)\E(Y|Z)$ и $\Var(X|Z)=\E(X^2|Z)-(\E(X|Z))^2$


Пример подсчета частной и условной корреляций.

Пример 1.

Закон распределения случайных величин $X_1$, $X_2$, $X_3$ задан двумя таблицами:

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{ccc}
& $X_3=0$ &    \\
& & \\
\cline{1-3}
 & $X_2=0$ & $X_2=1$ \\
\cline{1-3}
$X_1=0$ & $0.06$ & $0.08$ \\
$X_1=1$ & $0.24$ & $0.32$  \\
\end{tabular}
&
\begin{tabular}{ccc}
& $X_3=1$ &    \\
& & \\
\cline{1-3}
 & $X_2=0$ & $X_2=1$ \\
\cline{1-3}
$X_1=0$ & $0.1$ & $0$ \\
$X_1=1$ & $0$ & $0.2$ \\
\end{tabular}
\end{tabular}
\end{center}

Найдите условную корреляцию $\Corr(X_1,X_2|X_3)$ и частную корреляцию $\pCorr_{X_3}(X_1,X_2)$.

Решение.

Эти две таблички на самом деле реализуют простую мысль: при $X_3=1$ величины $X_1$ и $X_2$  связаны детерминистически линейно, а при $X_3=0$ величины $X_1$ и $X_2$ независимы.

Считаем две вспомогательные условные корреляции, $\Corr(X_1,X_2|X_3=0)=0$, $\Corr(X_1,X_2|X_3=1)=1$.

Отсюда получаем, что $\Corr(X_1,X_2|X_3)=X_3$. Для дискретных случайных величин запись условного ожидания не однозначна, и, например, ответ $\Corr(X_1,X_2|X_3)=X_3^2$ также будет верным.



Пример 2.

Величины $X_1$, $X_2$, $X_3$ имеют совместное нормальное распредлеение с математическим ожиданием $\E(X)=(1,2,-3)'$ и ковариационной матрицей
\[
\Var(X)=\begin{pmatrix}
9 & 2 & -1 \\
2 & 16 & 1 \\
-1 & 1 & 4
\end{pmatrix}
\]
Найдите условную корреляцию $\Corr(X_1,X_2|X_3)$ и частную корреляцию $\pCorr_{X_3}(X_1,X_2)$.

Решение.

\ldots

В данном примере частная и условная корреляция совпали. Это одно из приятных свойств многомерного нормального распределения:


\begin{theorem}
Если величины $X$, $Y$ и $Z$ имеют совместное нормальное распределение, то:
\begin{enumerate}
\item частная и условная дисперсии совпадают
\item частная и условная ковариации совпадают
\item частная и условная корреляции совпадают
\end{enumerate}
\end{theorem}


\section{Случай равных дисперсий}

Если дисперсии всех рассматриваемых (или достаточно только коррелируемых?) величин равны, то частная корреляция $Y$ и $X$ при фиксированном $Z$ равна стандартизованного коэффициенту регрессии, равна обычному коэффициенту регрессии $Y$ на $X$ и остальные, равна обычному коэффициенту регрессии $X$ на $Y$ и остальные.

К данному случаю относиться частная автокорреляционная функция для стационарных процессов:

Пример 3. AR(1) процесс



\section{Каноническая корреляция}

Если разбить все случайные величины на две группы, скажем на иксы и игреки, то каноническая корреляция --- это максимальная корреляция между линейной комбинацией иксов и линейной комбинацией игреков.

\[
canCorr(Y_1, Y_2, Y_3; X_1, X_2) = \max_{\alpha_1, \alpha_2, \beta_1, \beta_2, \beta_3} \Corr(\alpha_1 X_1 + \alpha_2 X_2, \beta_1 Y_1 + \beta_2 Y_2 +\beta_3 Y_3)
\]

Если группа игреков состоит из одной случайной величины, то мы получаем коэффициент множественной корреляции, квадрат которого можно назвать «теоретическим $R^2$».

\[
canCorr(Y; X_1, X_2, \ldots, X_k)
\]


И Шахерезаду застигло утро, и она прекратила дозволенные речи, заметив, что вышла замечательная статья Димы Борзых о методе канонических корреляций.


\section{Матрица --- Мать всех регрессий и корреляций}

Оказывается многие упомянутые меры зависимости порождаются обратной ковариационной матрицей.

Пусть $X$ --- это вектор случайных величин $X=(X_1, X_2, \ldots, X_k)'$ с ковариационной матрицей $C=\Var(X)$. Тогда обратная ковариационная матрица имеет следующую структуру:


\[
C^{-1} = \begin{pmatrix}
\frac{1}{pVar(X_1|X_{-1})} & &
 & \frac{1}{pVar(X_2|X_{-2})} &
 & & \frac{1}{pVar(X_3|X_{-3})}
\end{pmatrix}
\]

Похожий результат верен и для обратной корреляционной матрицы. Если $A$ --- корреляционная матрица, то:

\[
A^{-1} =
\begin{pmatrix}
\frac{\Var(X_1)}{pVar(X_1|X_{-1})} & & \\
 & \frac{\Var(X_2)}{pVar(X_2|X_{-2})} & \\
 & & \frac{\Var(X_3)}{pVar(X_3|X_{-3})} \\
\end{pmatrix}
\]


Частная корреляция между $X_i$ и $X_j$ при фиксированных остальных иксах равна:
\[
pCorr(X_i, X_j| \ldots ) = -\frac{A_{ij}}{\sqrt{A_{ii}A_{jj}}}
\]

Стандартизованный коэффициент при $X_j$ в регрессии $X_i$ на остальные иксы равен:
\[
\hat\beta_{ij}^{st} = -\frac{A_{ij}}{A_{ii}}
\]

Квадрат коэффициента множественной корреляции $X_i$ с остальными иксами равен:
\[
canCorr(X_i; \ldots ) = 1 - \frac{1}{A_{ii}}
\]




Доказательство:

\section{Частные корреляции бывают любые}

Произвольная матрица не может быть корреляционной матрицей вектора случайных величин. Она должна быть симметричной и неотрицательно определённой, а на диагонали должны стоять единицы.

Однако произвольная симметричная матрица из чисел в диапазоне $(-1; 1)$ может быть частной корреляционной матрицей. Более того, у стационарного процессам может быть совершенная любая частная автокорреляционная функция со значениями в диапазоне $(-1;1)$. Например, у стационарного процесса может быть  частная автокорреляционная функция
\[
\phi_{kk} = \begin{cases}
0, \text{ если } k = 1 \\
0.99, \text{ если } k = 2 \\
0, \text{ если } k \geq 3 \\
\end{cases}
\]

А вот и он, $y_t = 0.99 y_{t-2} + \varepsilon_t$:

<<>>=
library("forecast")
y <- arima.sim(n = 1000, model = list(ar = c(0, 0.99)))
tsdisplay(y)
@




\section{Почиташки}

https://en.wikipedia.org/wiki/Schur_complement#Applications_to_probability_theory_and_statistics

https://stats.stackexchange.com/questions/140080/why-does-inversion-of-a-covariance-matrix-yield-partial-correlations-between-ran

https://stats.stackexchange.com/questions/10795/how-to-interpret-an-inverse-covariance-or-precision-matrix

http://epublications.bond.edu.au/cgi/viewcontent.cgi?article=1147&context=ejsie

https://cran.r-project.org/web/packages/corpcor/corpcor.pdf

http://projecteuclid.org/euclid.aos/1176342881

\section{Выборочные характеристики}


это в статью два?



В теории обычную корреляцию и частную корреляцию можно посчитать, если известен закон распределения случайных величин. На практике закон распределения не известен, однако доступны наблюдения. Как по имеющимся наблюдениям оценить неизвестные корреляции?


Несколько способов оценки корреляции


Здесь пара картинок: википедийная с корреляциями и два ряда случайного блуждания/тренда



Несколько способов оценки частной корреляции


Оценкой для частной дисперсии $X_1$ при фиксированных $X_2$, \ldots, $X_n$ является оценка дисперсии случайной составляющей в регрессии $X_1$ на $X_2$, \ldots, $X_n$


Здесь про зависимости возникающие при оценке автокорреляционной и частной автокорреляционной функции для случайных процессов.


Несколько способов оценки множественной корреляции:

Если $canCorr(Y; X_1, X_2, \ldots, X_k) = 0$, то $\E(R^2_{adj})=0$.


Обратная к выборочной ковариационной матрице

\[
\hat C^{-1} = \begin{pmatrix}
\frac{1}{\hat \sigma^2_1} & &
 & \frac{1}{\hat \sigma^2_2} &
 & & \frac{1}{\hat \sigma^2_3}
\end{pmatrix}
\]

Если мы предположим, что разные наблюдения в выборке независимы и одинаково распределены, то получим такие выборочные оценки

\begin{tabular}{lll} \toprule
Теоретический & Выборочный & Коммент \\
\midrule
$pVar(y_i|z_i)$      & $\hat \sigma_u^2$ в регрессии $y$ на $z$, выборочная дисперсия остатков & несмещённость? \\
$pCov(y_i, x_i|z_i)$ & выборочная ковариация остатков двух регрессий & несмещённость \\
$canCorr(y_i; x_{i1}, x_{i2}, \ldots, x_{ik})$ & $R^2$, $R^2_{adj}$ &  \\
коэффициент в множественной регрессии & $\hb = (X'X)^{-1}X'y$ & несмещенный
стандартизированный коэффициент в множественной регрессии & $\hb_j^{st}=\hb_j \frac{\hat\sigma_x}{\hat\sigma_y}$, $\hb^{st} = (X^{st\prime}X^{st})^{-1}X^{st\prime}y^{st}$ & несмещенный? \\
обычная корреляция & $sCorr(y, x) = \frac{\sum (x_i - \bar x)(y_i - \bar y)}{\sqrt{...}}$ & \\
частная корреляция & корреляция остатков регрессии, обращение корреляционной матрицы & \\
\bottomrule
\end{tabular}



\end{document}
