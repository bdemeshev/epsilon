\documentclass[10pt]{article}

\usepackage{epsilonj}

\usepackage{epigraph}
\usepackage{tikz}
\usepackage{bbm}
\usetikzlibrary{calc,intersections,decorations.pathreplacing}
\usetikzlibrary{arrows.meta}

\usepackage{tikz-3dplot}

% этот код считает отмечает углы в tikz
\newcommand\markangle[6]{% origin X Y radius radiusmark mark
  % fill red circle
  \begin{scope}
    \path[clip] (#1) -- (#2) -- (#3);
    \fill[color=red,fill opacity=0.5,draw=red,name path=circle]
    (#1) circle (#4);
  \end{scope}
  % middle calculation
  \path[name path=line one] (#1) -- (#2);
  \path[name path=line two] (#1) -- (#3);
  \path[%
  name intersections={of=line one and circle, by={inter one}},
  name intersections={of=line two and circle, by={inter two}}
  ] (inter one) -- (inter two) coordinate[pos=.5] (middle);
  % put mark
  \node at ($(#1)!#5!(middle)$) {#6};
}

\RequirePackage{graphicx}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\II}{\mathbbm{1}}
\def \hb{\hat{\beta}}
\DeclareMathOperator{\pCorr}{pCorr}
\DeclareMathOperator{\canCorr}{canCorr}
\DeclareMathOperator{\pCov}{pCov}
\DeclareMathOperator{\pVar}{pVar}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\sVar}{sVar}
\DeclareMathOperator{\Dep}{Dep}
\DeclareMathOperator{\Lin}{Lin}
\renewcommand{\P}{\mathbb{P}}


\begin{document}

\TITLE{Матрица-мать всех регрессий}
\SHORTTITLE{Матрица-мать всех регрессий}

\AUTHOR{Борис Демешев}{НИУ ВШЭ, Москва.}
\SHORTAUTHOR{Борис Демешев}

\DoFirstPageTechnicalStuff


\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Определение}


\begin{abstract}
Корреляция — это способ описать силу линейной зависимости между двумя случайными величинами одним числом. Каков геометрический смысл корреляции? Что такое частная корреляция? Как связаны частная и условная корреляция? Как связаны частные корреляции, коэффициенты регрессии и стандартизованные коэффициенты регрессии? И, главное, оказывается, существует матрица-мать, порождающая все эти меры зависимости.
\end{abstract}

\begin{keyword}
корреляция, частная корреляция, условная корреляция, косинус, проекция, стандартизованный коэффициент, теорема Фриша-Во-Ловелла, матрица-мать всех регрессий
\end{keyword}


\section{Сколько вешать в граммах?}

% Люди часто хотят найти простое решение сложной задачи.

\epigraph{Everything Should Be Made as Simple as Possible, But Not Simpler.}{\textit{Roger Sessions, \\ paraphrase of Albert Einstein}}


\epigraph{There is always a well-known solution to every human problem — neat, plausible, and wrong.}{\textit{Henry Louis Mencken}}




Почему при болезни мы измеряем температуру тела? По всей видимости важны два факта:

\begin{enumerate}
\item Измерить температуру очень удобно
\item Это измерение несёт в себе информацию о здоровье
\end{enumerate}

Сложное описание состояния здоровья сводится измерением температуры к одной цифре. Естественно, куча информации теряется в этой цифре и бессмысленно лечить человека, руководствуясь только температурой его тела. Температура $40^{\circ}$ говорит, что что-то не так, но что — непонятно. А температура $36.6^{\circ}$ ещё не говорит о том, что у человека идеальное здоровье. Однако процедура очень проста и в некоторых ситуациях, например, при обыкновенной простуде, её достаточно для принятия решения о приёме жаропонижающего.

Если бы для измерения температуры нужно было специальное устройство размером с половину комнаты, никто бы дома её не мерял. Люди любят упрощать, а здесь простота измерения очень важна!

Подобным образом дела обстоят и с описанием зависимости между случайными величинами. Зависимость между случайными величинами полностью описывается их совместной функцией распределения, $F(x,y)=\P(X\leq x, Y\leq y)$. Это слишком сложно! Вместо сложной функции распределения мы хотим получить одно число. Некую «силу зависимости». Назовём это мифическое число $\Dep(X,Y)$.

Чего бы нам хотелось от этого числа?
\begin{enumerate}
\item Посчитать это число очень удобно
\item Это число несёт в себе информацию о зависимости
\end{enumerate}

% В случае двух случайных величин наиболее популярными мерами зависимости являются ковариация, корреляция и коэффициент в парной регрессии.

\section{Ковариация}

В каком смысле «удобно» считать?

Очень часто возникают суммы случайных величин, поэтому было бы здорово, чтобы у суммы легко считалась наша характеристика $\Dep(X+Z,Y)$. Проще всего было бы, если бы:

\[
\Dep(X+Z,Y)=\Dep(X,Y)+\Dep(Z,Y)
\]

Кроме того, хотелось бы, чтобы сила зависимости $X$ и $Y$ была бы такой же, как и сила зависимости $Y$ и $X$:

\[
\Dep(X, Y) = \Dep(Y, X)
\]



И, конечно, мы ждём, что у независимых случайных величин нулевая сила зависимости, $\Dep(X,Y)=0$, а ненулевая сила зависимости, $\Dep(X,Y)\neq 0$, была бы возможна только у зависимых случайных величин.

Этим трём свойствам, аддитивности, симметричности и занулению при независимости, удовлетворяет ковариация.

\begin{definition}
Ковариация величин $X$ и $Y$ равна
\[
\Cov(X,Y)=\E\left[  (X-\E(X)) (Y-\E(Y))   \right] = \E(XY)-\E(X)\E(Y)
\]
\end{definition}

здесь рассказать про прямоугольники и площадь с плюсом/минусом? почему она такая?

Можно доказать, что любая другая непрерывная функция удовлетворяющая трём заявленным свойствам обязана быть пропорциональна ковариации (?).



\section{Коэффициенты в парной регрессии}

Наиболее легко интерпретировать коэффициенты в парной регрессии. Пусть у нас есть две случайных величины $X_1$ и $X_2$. Мы можем получить два коэффициента:

\begin{itemize}
\item коэффициент $\beta_{12}$ — на сколько в среднем растёт $X_1$ при росте $X_2$ на единицу
\item коэффициент $\beta_{21}$ — на сколько в среднем растёт $X_2$ при росте $X_1$ на единицу
\end{itemize}


А теперь из этого словесного определения мы получим формулу для $\beta_{12}$. Разложим величину $X_1$ на два слагаемых. Первое слагаемое вбирает в себя всю ту часть $X_1$, которая линейно зависит от $X_2$, а второе — всё оставшееся:

% \[
% \frac{Y}{\sigma_Y}=\rho \cdot \frac{X}{\sigma_X} + \varepsilon
% \]
%
% В этой формуле видно, что с ростом $X$ на одно стандартное отклонений $\sigma_X$ правая часть изменится в среднем на $\rho$, и, следовательно, величина $Y$ в среднем изменится на $\rho \cdot \sigma_Y$.

\[
X_1=\beta_{12} \cdot X_2 + u
\]

Здесь $\beta_{12}$ показывает на сколько единиц в среднем растёт $X_1$ при росте $X_2$ на одну единицу.

Естественно, мы хотим, чтобы с ростом $X_2$ величина $u$ в среднем не менялась, то есть хотим нулевую «силу зависимости» между ними, $\Cov(X_2, u)=0$.


\[
\Cov\left(X_2, X_1 - \beta_{12} X_2 \right) = 0
\]

По свойствам ковариации получаем

\[
\Cov(X_2, X_1) - \beta_{12} \Cov(X_2,X_2)=0
\]

И, тадам, выражаем коэффициент $\beta_{12}$:

\[
\beta_{12} = \frac{\Cov(X_1, X_2)}{\Var(X_2)}
\]

По аналогии окажется, что
\[
\beta_{21} = \frac{\Cov(X_1, X_2)}{\Var(X_1)}
\]


Заметим, что хотя знак $\beta_{12}$ и знак $\beta_{21}$ совпадают, сами они будут совпадать только если $\Var(X_1) = \Var(X_2)$.


Коэффициент $\beta_{12}$, естественно, будет реагировать на изменение единиц измерения $X_1$ или $X_2$. Если увеличить $X_1$ в 10 раз, то и $\beta_{12}$ увеличиться в 10 раз. Иногда чувствительность к единицам измерения является нежелательной.

Коэффициент регрессии можно избавить от чувствительности к единицам измерения, если стандартизировать исходные переменные, то есть поделить их на стандартные отклонения. При этом мы получим стандартизированный коэффициент регрессии:

\[
\beta_{12}^{st} = \frac{\Cov(X_1/\sigma_1, X_2/\sigma_2)}{\Var(X_2/\sigma_2)} =  \beta_{12} \frac{\sigma_2}{\sigma_1} = \frac{\Cov(X_1,X_2)}{\sqrt{\Var(X_1)\Var(X_2)}}
\]

По своему смыслу $\beta_{12}^{st}$ показывает, на сколько своих стандартных отклонений в среднем изменяется $X_1$ при изменении $X_2$ на одно своё стандартное отклонение.

Заметим, что произошло чудо! В силу симметричности выражения для $\beta_{12}^{st}$ оказывается, что $\beta_{12}^{st}=\beta_{21}^{st} = \beta^{st}$. Получается, с одной стороны, что:
\[
\frac{X_1}{\sigma_1} = \beta^{st}\frac{X_2}{\sigma_2} + u_1
\]
А с другой стороны, в противоположном разложении будет тот же коэффициент:
\[
\frac{X_2}{\sigma_2} = \beta^{st}\frac{X_1}{\sigma_1} + u_2
\]


Это чудо слегка контр-интуитивно! Если бы зависимость между $X_1$ и $X_2$ была бы жёстко детерминистически линейной, и с ростом $X_2$ на единицу величина $X_1$ росла бы на $\Delta$, то с ростом $X_1$ на единицу величина $X_2$ росла бы на $1/\Delta$. Для случайных величин обращения не происходит! Если с ростом $X_1$ на одно своё стандартное отклонение величина $X_2$ в среднем растёт на $\Delta$ своих стандартных отклонений, то и с ростом $X_2$ на одно своё стандартное отклонение величина $X_1$ в среднем растёт на $\Delta$ своих стандартных отклонений.

Здесь задачи от Фрэнсиса Галтона?

\section{Корреляция по-русски}

Обычно в учебниках даётся такое определение корреляции

\begin{equation}
\label{def_corr}
\Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}.
\end{equation}

Естественно, возникает вопрос: «С какого перепугу? Почему это мы делим ковариацию на что-то там?»

Мы дадим естественное определение корреляции:

\begin{definition}
Корреляция величин $X_1$ и $X_2$ — это среднее геометрическое среднего изменения $X_1$ при росте $X_2$ на единицу и среднего изменения $X_2$ при росте $X_1$ на единицу, совпадающее знаком с этими изменениями:

\[
\Corr(X, Y) = \sqrt{\beta_{XY}\beta_{YX}} \cdot \sign(\beta_{XY})
\]

\end{definition}

Тут может возникнуть естественный вопрос: а почему именно среднее геометрическое, а не более привычное среднее арифметическое? Проблема со средним арифметическим становится понятна, если посмотреть на единицы измерения. Если $\beta_{12}$ измеряется в килограммах на сантиметр, то $\beta_{21}$ будет измеряться в сантиметрах на килограмм. И тогда совершенная ерунда получается, если считать среднее арифметическое. А среднее геометрическое автоматически оказывается безразмерной величиной.


Два определения корреляции полностью эквивалентны, действительно:
\[
\Corr(X, Y)^2 = \frac{\Cov(X,Y)^2}{\Var(X)\Var(Y)} = \beta_{12} \cdot \beta_{21}
\]

Посмотрим, как коэффициент корреляции связан со стандартизованными коэффициентам регрессии:
\[
\beta_{12} \cdot \beta_{21} = \beta_{12}^{st} \cdot \beta_{21}^{st}= (\beta_{12}^{st})^2 = (\beta_{21}^{st})^2
\]

То есть коэффициент корреляции просто равен стандартизованному коэффициенту бета в регрессии! Поэтому корреляция между величинами $X$ и $Y$ также показывает, на сколько своих стандартных отклонений в среднем изменяется $X$ при изменении $Y$ на одно своё стандартное отклонение.


\section{Геометрический смысл корреляции}

Давайте рисовать случаные величины векторами-стрелочками! Не в том смысле, что у стрелочки случайное направление или длина, а в том смысле, что направление и длина стрелочки описывают характеристики этой случайной величины.

Любую геометрию можно задать, задав скалярное произведение. Действительно, если мы умеем считать скалярное произведение двух любых векторов, $\langle\vec{a},\vec{b}\rangle$, то длина вектора считается ровно как в 9-м классе:

\[
|\vec{a}|=\sqrt{\langle\vec{a},\vec{a}\rangle}
\]

И также любой девятикласник помнит, что косинус угла между векторами считается как

\[
\cos(\vec{a},\vec{b})=\frac{\langle\vec{a},\vec{b}\rangle}{|\vec{a}||\vec{b}|}
\]

Мы определим скалярное произведение двух случайных величин как их ковариацию:

\[
\langle X,Y\rangle=\Cov(X,Y)
\]

При таком подходе длиной случайной величины окажется стандартное отклонение:

\[
\sqrt{\Cov(X,X)}=\sqrt{\Var(X)}=\sigma_X
\]

А корреляция окажется косинусом угла между случайными величинами:
\[
\cos \phi =\cos(X,Y)=\frac{\Cov(X,Y)}{\sigma_X\sigma_Y}=\Corr(X,Y)
\]

\begin{center}
\begin{tikzpicture}
\path (0,0) coordinate (origin);
\path (origin) ++(1,2) coordinate (Y);
\path (origin) ++(3,0) coordinate (X);
\draw[-{Latex[length=3mm]},thick] (origin) -- (Y);
\draw[-{Latex[length=3mm]},thick] (origin) -- (X);
\node [above] at (X) {$X$};
\node [right] at (Y) {$Y$};
\markangle{origin}{X}{Y}{6mm}{4mm}{$\phi$}
\end{tikzpicture}
\end{center}

Значит в нашей геометрии  длина стрелочки — стандартное отклонение случайной величины, а косинус угла между двумя стрелочками — это корреляция двух случайных величин. Дисперсия, следовательно, это квадрат длины случайной величины. Перпендикулярными случайными величинами будут те, косинус угла между которыми равен нулю, то есть некоррелированные. Автоматически оказывается, что корреляция лежит в диапазоне от $(-1)$ до $1$.

Например, сформулируем в данной геометрии теорему Пифагора. Если случайные величины $X$ и $Y$ перпендикулярны (корреляция или ковариация равны нулю), то дисперсия их суммы (квадрат длины гипотенузы) равна сумме их дисперсий (сумму квадратов длин катетов):

\[
\Var(X+Y)=\sigma^2_{X+Y}=\sigma^2_X+\sigma^2_Y=\Var(X)+\Var(Y)
\]

\begin{center}
\begin{tikzpicture}
\path (0,0) coordinate (origin);
\path (origin) ++(4,2) coordinate (Y);
\path (origin) ++(4,0) coordinate (X);
\draw[-{Latex[length=3mm]},thick] (origin) -- (Y);
\draw[-{Latex[length=3mm]},thick] (origin) -- (X);
\draw[-{Latex[length=3mm]},thick] (X) -- (Y);
\node [yshift=10pt,xshift=-15pt] at (X) {$X$};
\node [yshift=-15pt,xshift=15pt] at (Y) {$Y$};
\node [yshift=5pt,xshift=-30pt] at (Y) {$X+Y$};
\draw [decorate,decoration={brace,amplitude=10pt}] (X) -- (origin) node [black,midway,yshift=-15pt] {\footnotesize $\sigma_X$};
\draw [decorate,decoration={brace,amplitude=10pt}] (Y) -- (X) node [black,midway,xshift=20pt] {\footnotesize $\sigma_{Y}$};
\draw [decorate,decoration={brace,amplitude=10pt}] (origin) -- (Y) node [black,midway,yshift=15pt,xshift=-10pt] {\footnotesize $\sigma_{X+Y}$};
\end{tikzpicture}
\end{center}

Введение геометрии позволяет говорить о проекции. Например, можно спроецировать случайную величину $Y$ на множество случайных величин пропорциональных величине $X$, $\{cX | c\in \RR \}$. Если на обычной плоскости спроецировать вектор $\vec{a}$ на прямую, порожденную вектором $\vec{b}$, то получится $\cos(\vec{a},\vec{b})\cdot \vec{b}$. По аналогии, если спроецировать случайную величину $Y$ на множество $\{cX | c\in \RR \}$, то получится $\Corr(X,Y)\cdot X$. Другими словами, среди случайных величин пропорциональных $X$ величина  $\hat{Y}=\Corr(X,Y)\cdot X$ — самая похожая на величину $Y$.

Понятие проекции позволяет интепретировать квадрат корреляции. Квадрат косинуса равен отношению квадрата длины прилежащего катета $\Var(\hat{Y})$ к квадрату гипотенузы $\Var(Y)$.

(картинка)

Следовательно, $\Corr(X,Y)^2=\frac{\Var(\hat{Y})}{\Var(Y)}$, то есть квадрат корреляции показывает долю дисперсии $Y$, которую можно объяснить с помощью величин пропорциональных $X$.

\section{Великое равенство}

Если собрать воедино все кусочки, то мы доказали, что для двух случайных величин $X_1$ и $X_2$:
\[
\Corr^2(X_1, X_2) = \beta_{12} \cdot \beta_{21} = \beta_{12}^{st} \cdot \beta_{21}^{st}= (\beta_{12}^{st})^2 = (\beta_{21}^{st})^2 = \frac{\Var(\hat X_1)}{\Var(X_1)}=\frac{\Var(\hat X_2)}{\Var(X_2)}
\]


\section{Корреляция и независимость}

\begin{theorem}
Случайные величины $X$ и $Y$ независимы тогда и только тогда, когда некоррелированы любые функции $f(X)$ и $g(Y)$.
\end{theorem}

Другими словами для независимости $X$ и $Y$ необходима некоррелированность пар $X$ и $Y$, $X^2$ и $\cos(Y)$, $\exp(X)$ и $1/Y$, и так далее. Из этого следует, что некоррелированность $X$ и $Y$ является необходимым, но недостаточным условием для независимости.

Можно выделить три «степени» независимости случайных величин $X$ и $Y$:

\begin{tabular}{rl}
\toprule
Некоррелированность $Y$ и $X$ & $\Corr(X,Y)=0$ \\
$\E(Y|X)=\E(Y)$ & $\Corr(f(X),Y)=0$ для всех $f()$ \\
Независимость $Y$ и $X$ & $\Corr(f(X),g(Y))=0$ для всех $f()$ и $g()$ \\
\bottomrule
\end{tabular}



Многие ошибочно считают, что если величина $X$ имеет нормальное распределение $N(\mu_X, \sigma^2_X)$ и величина $Y$ имеет нормальное распределение $N(\mu_Y, \sigma^2_Y)$, и $X$ и $Y$ некоррелированы, то они независимы. Это неверно.

Контрпример. Случайная величина $X$ имеет стандартное нормальное распределение $N(0;1)$, случайная величина $Z$ независима от $X$ и равновероятно принимает значения $-1$ и $+1$. Определим величину $Y$ как их произведение, $Y=XZ$.

В этом примере величины $X$ и $Y$ зависимы, так как $|X|=|Y|$. Однако $Y$ распределена нормально стандартно и $\Corr(X,Y)=0$.

Правильная теорема звучит так:

\begin{theorem}
Если некоррелированные случайные величины $X$ и $Y$ имеют совместное нормальное распределение, то $X$ и $Y$ независимы.
\end{theorem}

Попутно упомянем ещё одно неожиданное свойство предъявленного контрпримера. Если случайные величины нормальны по отдельности, то вполне возможно, что их сумма ненормальна. Для пары величин, имеющих совместное нормальное распределение, это невозможно.



\section{Более двух случайных величин}

Если случайных оказывается больше двух, то для описания зависимости с помощью одного числа оказывается очень много вариантов. Среди популярных можно назвать:
\begin{enumerate}
\item коэффициент бета в множественной регрессии
\item стандартизированный коэффициент множественной регрессии
\item частная корреляция
\item каноническая корреляция
\end{enumerate}


\section{Стандартизованный коэффициент регрессии}

Для нахождения стандартизованного коэффициента множественной регрессии используется разложение
\[
\frac{Y}{\sigma_Y}=\rho_{XY;Z} \cdot \frac{X}{\sigma_X} + \rho_{YZ;X} \frac{Z}{\sigma_Z} + u
\]

Два подхода к определению стандартизованного коэффициента регрессии эквивалентны в силу теоремы Фриша-Во-Ловелла (Frisch–Waugh–Lovell). Обычно эта теорема формулируется применительно к регрессии, а здесь мы приведём её вариант для случайных величин.

\begin{theorem}
Если имеют место разложения:
\[
Y=a_1 Z_1 + a_2 Z_2 + \ldots + a_n Z_n + \tilde{Y}, \text{ где }\; \tilde{Y}\bot Z_1, Z_2, \ldots, Z_n
\]
и
\[
X=b_1 Z_1 + b_2 Z_2 + \ldots + b_n Z_n  + \tilde{X}, \text{ где }\; \tilde{X}\bot Z_1, Z_2, \ldots, Z_n
\]
То в разложениях
\[
\tilde{Y}=d \tilde{X} + \varepsilon, \text{ где }\; \varepsilon \bot \tilde{X}
\]
и
\[
Y=c_1 Z_1 + c_2 Z_2 + \ldots + c_n Z_n  + dX + u, \text{ где }\; u \bot Z_1, Z_2, \ldots, Z_n, X
\]
коэффициенты при $\tilde{X}$ и $X$ совпадают.
\end{theorem}


\section{Частная корреляция}

Есть два словесных определения частной корреляции. Первое — через коэффициенты в регрессиях:

\begin{definition}
Частная корреляция между величинами $X_1$ и $X_2$ при фиксированной величине $X_3$ — это среднее геометрическое между коэффициентом при $X_2$ в регрессии $X_1$ на остальные переменные и коэффициентом при $X_1$ в регрессии $X_2$ на остальные переменные, совпадающее знаком с этими коэффициентами.
\end{definition}

Попутно первое определение использует нетривиальных факт, что знак при $X_1$ в регрессии $X_2$ на остальные переменные и знак при $X_2$ в регрессии $X_1$ на остальные переменные совпадают.

Второе определение — через остатки и обычную корреляцию:

\begin{definition}
Частная корреляция между величинами $X_1$ и $X_2$ при фиксированной величине $X_3$ — это есть обычная корреляция между остатками регрессии $X_1$ на $X_3$ и остатками регрессии $X_2$ на $X_3$.
\end{definition}

Согласно второму определению корреляция показывает на сколько своих стандартных отклонений в среднем вырастет величина $Y$, очищенная от связи с $Z$, при росте величины $X$, очищенной от связи с $Z$, на одно своё стандартное отклонение.


Второе определение даёт следующий подход к подсчёту частной корреляции:
\begin{enumerate}
\item Спроецируем $X$ на множество величин, некоррелированных с $Z$. Получим $\tilde{X}$.
\item Спроецируем $Y$ на множество величин, некоррелированных с $Z$. Получим $\tilde{Y}$.
\item Частная корреляция между $X$ и $Y$ при фиксированной $Z$ — это обычная корреляция между $\tilde{X}$ и $\tilde{Y}$.
\end{enumerate}


доказательство эквивалентности подоходов?


Второе определение можно дополнить неожиданной теоремой:

\begin{theorem}
Частная корреляция между величинами $X_1$ и $X_2$ при фиксированной величине $X_3$ — это есть обычная корреляция между остатками регрессии $X_1$ на $X_2$, $X_3$ и остатками регрессии $X_2$ на $X_1$, $X_3$, взятая с противоположным знаком.
\end{theorem}


Геометрическое доказательство:

\tdplotsetmaincoords{70}{-50}
\begin{tikzpicture}[tdplot_main_coords]
    \draw[thick,->] (0,0,0) -- (3,0,0) node[anchor=east]{$X_3$};
    \draw[thick,->] (0,0,0) -- (1,3,0) node[anchor=north west]{$X_1$};
    \draw[thick,->] (0,0,0) -- (1,1,2) node[anchor=south]{$X_2$};

    \draw[dashed] (1,1,2) -- (1,0,0);
    \draw[dashed] (1,3,0) -- (1,0,0);
    \draw[dashed] (1,1,2) -- (1,1,0);
    \draw[dashed] (1,3,0) -- (1,0.6,1.2);
\end{tikzpicture}


\section{Частная дисперсия и ковариация}

Определения частной корреляции можно сформулировать с помощью частной ковариации и частной дисперсии.

\[
\pCorr(X, Y ; Z) = \frac{\pCov(X,Y ; Z)}{\sqrt{\pVar(X ; Z)\pVar(Y ; Z)}}
\]

Частная дисперсия величины $X$ очищенной от $Z$ определяется как

\[
\pVar(X ; Z) = \Var(X-aZ), \text{ где  константа } a \text{ такова, что} \Cov(X-cZ, Z) = 0
\]

Частная ковариация

\[
\pCov(X,Y ; Z) = \Cov(X-aZ, Y- bZ ), \text{ где  константы } a \text{ и } b \text{ такова, что} \Cov(X-aZ, Z) = \Cov(Y-bZ, Z) = 0,
\]


\section{Условная корреляция}

\begin{definition}
Условная корреляция между величинами $X$ и $Y$ при известном значении величины $Z$ показывает на сколько своих стандартных отклонений $\sigma_Y$ в среднем вырастет $Y$ при росте величины $X$ на одно своё стандартное отклонение $\sigma_X$ при заданном значении величины $Z$.
\end{definition}


Следует подчеркнуть одно существенное отличие условной корреляции от обычной и частной. Обычная и частная корреляция являеются константами. Условная корреляция $\Corr(X,Y|Z)$ является функцией от $Z$. Величина $Z$ является случайной, поэтому и условная корреляция $\Corr(X,Y|Z)$ является случайной величиной.

Здесь регрессионное определение????

Чуть более формальное определение:

\begin{definition}
\[
\Corr(X,Y|Z) = \frac{\Cov(X,Y|Z)}{\sqrt{\Var(X|Z)\Var(Y|Z)}},
\]
\end{definition}

где $\Cov(X,Y|Z)=\E(XY|Z)-\E(X|Z)\E(Y|Z)$ и $\Var(X|Z)=\E(X^2|Z)-(\E(X|Z))^2$


Пример подсчета частной и условной корреляций.

Пример 1.

Закон распределения случайных величин $X_1$, $X_2$, $X_3$ задан двумя таблицами:

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{ccc}
& $X_3=0$ &    \\
& & \\
\cline{1-3}
 & $X_2=0$ & $X_2=1$ \\
\cline{1-3}
$X_1=0$ & $0.06$ & $0.08$ \\
$X_1=1$ & $0.24$ & $0.32$  \\
\end{tabular}
&
\begin{tabular}{ccc}
& $X_3=1$ &    \\
& & \\
\cline{1-3}
 & $X_2=0$ & $X_2=1$ \\
\cline{1-3}
$X_1=0$ & $0.1$ & $0$ \\
$X_1=1$ & $0$ & $0.2$ \\
\end{tabular}
\end{tabular}
\end{center}

Найдите условную корреляцию $\Corr(X_1,X_2|X_3)$ и частную корреляцию $\pCorr(X_1,X_2 ; X_3)$.

Решение.

Эти две таблички на самом деле реализуют простую мысль: при $X_3=1$ величины $X_1$ и $X_2$  связаны детерминистически линейно, а при $X_3=0$ величины $X_1$ и $X_2$ независимы.

Считаем две вспомогательные условные корреляции, $\Corr(X_1,X_2|X_3=0)=0$, $\Corr(X_1,X_2|X_3=1)=1$.

Отсюда получаем, что $\Corr(X_1,X_2|X_3)=X_3$. Для дискретных случайных величин запись условного ожидания не однозначна, и, например, ответ $\Corr(X_1,X_2|X_3)=X_3^2$ также будет верным.



Пример 2.

Величины $X_1$, $X_2$, $X_3$ имеют совместное нормальное распредлеение с математическим ожиданием $\E(X)=(1,2,-3)'$ и ковариационной матрицей
\[
\Var(X)=\begin{pmatrix}
9 & 2 & -1 \\
2 & 16 & 1 \\
-1 & 1 & 4
\end{pmatrix}
\]
Найдите условную корреляцию $\Corr(X_1,X_2|X_3)$ и частную корреляцию $\pCorr(X_1, X_2 ; X_3)$.

Решение.

\ldots

В данном примере частная и условная корреляция совпали. Это одно из приятных свойств многомерного нормального распределения:


\begin{theorem}
Если величины $X$, $Y$ и $Z$ имеют совместное нормальное распределение, то:
\begin{enumerate}
\item частная и условная дисперсии совпадают
\item частная и условная ковариации совпадают
\item частная и условная корреляции совпадают
\end{enumerate}
\end{theorem}


Прикольный пример. $X \sim U[0;1]$. При фиксированном $X$ величины $Y$ и $Z$ условно независимы и одинаково распределены $Y|X \sim U[0; X^2]$ и $Z|X \sim U[0;X^2]$. В данном случае условная корреляция $\Corr(Y,Z | X) = 0$, однако $\pCorr(Y, Z ; X) \neq 0$.

This example was suggested by P. Groeneboom, and published in Kurowicka (2001).

\[
\pCorr(Y, Z ; X) =\frac{1}{13}
\]



\section{Случай равных дисперсий}



Если дисперсии двух рассматриваемых величин $X_1$ и $X_2$ равны, то стандартизированный коэффициент регрессии равен обычному коэффициенту регрессии, $\beta_{12}=\beta_{12}^{st}$.

Если же выполнено более сильное условие стационарности случайного процесса $X_1$, $X_2$, \ldots, то
\[
\pCorr(X_1, X_k ; X_{2,3, \ldots, (k-1)}) = \beta_{1k} = \beta_{k1} = \beta_{1k}^{st} = \beta_{k1}^{st}
\]

К данному случаю относится частная автокорреляционная функция для стационарных процессов:

Пример 3. AR(1) процесс

\[
y_t = 0.7 y_{t-1} + u_t
\]

Найдите $\pCorr(y_t, y_{t-3} ; y_{t-1}, y_{t-2})$.

Частная корреляция в данном случае обычному коэффициенту регрессии, то есть коэффициенту перед $y_{t-3}$ в регрессии $y_{t}$. Замечаем, что
\[
y_t = 0.7 y_{t-1} + 0 y_{t-2} + 0 y_{t-3} + u_t
\]

Значит,  $\pCorr(y_t, y_{t-3} ; y_{t-1}, y_{t-2}) = 0$.




\section{Каноническая корреляция}

Если разбить все случайные величины на две группы, скажем на иксы и игреки, то каноническая корреляция — это максимальная корреляция между линейной комбинацией иксов и линейной комбинацией игреков.

\[
\canCorr(Y_1, Y_2, Y_3; X_1, X_2) = \max_{\alpha_1, \alpha_2, \beta_1, \beta_2, \beta_3} \Corr(\alpha_1 X_1 + \alpha_2 X_2, \beta_1 Y_1 + \beta_2 Y_2 +\beta_3 Y_3)
\]

Если группа игреков состоит из одной случайной величины, то мы получаем коэффициент множественной корреляции, квадрат которого можно назвать «теоретическим $R^2$».

\[
\canCorr(Y; X_1, X_2, \ldots, X_k)
\]

Каноническая корреляция — это замечательное оправдание для скорректированного коэффициента $R^2_{adj}$. А именно:

Если $\canCorr(Y; X_1, X_2, \ldots, X_k) = 0$, то $\E(R^2_{adj})=0$.


И Шахерезаду застигло утро, и она прекратила дозволенные речи, заметив, что вышла замечательная статья Димы Борзых о методе канонических корреляций.


\section{О двойственности}

В математике часто встречаются двойственные преобразования. Если двойственное волшебство $f$ применить два раза, то получится исходный объект, то есть $f(f(A)) = A$.

- Положительные числа и $f(x)= 1/x$. Действительно, $f(f(x))=x$.

- Матрицы и транспонирование. Конечно же, $(A^T)^T=A$.

- Случайные величины с положительной дисперсией и $f(X) =  X/\Var(X)$. Убедитесь самостоятельно, что $f(f(X))=X$.

- Папа как двойственное преобразование орущего младенца.

Объекты $A$ и $f(A)$ также называют двойственными. Приведите свои примеры двойственных преобразований.

\section{Матрица—Мать всех регрессий и корреляций}

Оказывается есть матрица $M$, глянув на которую, можно с помощью пары арифметических действий посчитать любой коэффициент в любой регрессии.

Пусть $X$ — это вектор случайных величин $X=(X_1, X_2, \ldots, X_k)'$ с ковариационной матрицей $C=\Var(X)$.


От величин $X_1$, $X_2$, \ldots, $X_k$ мы перейдём к двойственным величинам $X_1^*$, $X_2^*$, \ldots, $X_k^*$. Чтобы перейти от $\{X_i\}$ к $\{X_i^*\}$ надо применить ровно то же преобразование, что и для перехода от $\{X_i^*\}$ к $\{X_i\}$. Сейчас мы это преобразование опишем.


По своей сути $X_i^*$ будет пропорциональна той части $X_i$, которая не коррелирована ни с одним оставшимся $X_j$. Строить величину $X_i^*$ мы будем как линейную комбинацию $X_1$, $X_2$, \ldots, $X_k$ с некими весами $\mu_{ij}$,
\[
X_i^* = \sum_{t=1}^k \mu_{it} X_t
\]

Коэффициенты $\mu_{ik}$ должны быть такими, чтобы выполнялось условие:
\[
\Cov(X_i^*, X_j) = \begin{cases}
1, \text{ при } i = j \\
0, \text{ при } i\neq j \\
\end{cases}
\]

Из этой системы видна двойственность $\{X_i^*\}$ и $\{X_i\}$. Если поменять «звезданутые» $X_i$ и исходные $X_i$, система уравнений ни капли не изменится.


Первый смысл коэффициента $\mu_{it}$ — вес, с которым $X_t$ входит в величину $X_i^*$. Заметим, что есть и другая интерпретация, а именно, эти коэффициенты показывают ковариацию между новыми $X_i^*$:

\[
\Cov(X_i^*, X_j^*)= \Cov(X_i^*, \sum_{t} \mu_{jt} X_t) = \mu_{ji}
\]

Из этого, в частности, следует, что $\mu_{ij}=\mu_{ji}=\Cov(X_i^*, X_j^*)$ и $\mu_{ii} = \Var(X_i^*)$.


Из условия:
\[
X_i^* = \sum_{t=1}^k \mu_{ik} X_t
\]

Следует также представление:
\[
X_i = \sum_{t \neq i} - \frac{\mu_{ik}}{\mu_{ii}}X_t  + \frac{1}{\mu_{ii}}X_i^*
\]

То есть $\frac{1}{\mu_{ii}}X_i^*$ — это остаток от регрессии $X_i$ на остальные $X_j$. Заметим попутно, что дисперсия этого остатка равна
\[
\Var\left( \frac{1}{\mu_{ii}}X_i^* \right) = \frac{\mu_{ii}}{\mu_{ii}^2} = \frac{1}{\mu_{ii}}
\]


Выпишем, как связаны коэффициенты $\mu_{ij}$, ковариации между новыми $X_i^*$, со ковариациями между $X_i$:
\begin{equation}
\Cov(X_i^*, X_j) = \sum_{k=1}^n \mu_{ik}\Cov(X_k, X_j) = \begin{cases}
1, \text{ при } i = j \\
0, \text{ при } i\neq j \\
\end{cases}
\end{equation}


В матричном виде это же соотношение записывается потрясающе просто:
\[
M\cdot C = I
\]
То есть $M=C^{-1}$. Таким образом получается, что обратная ковариационная матрица позволяет легко посчитать коэффициенты любой регрессии!


Например, мы хотим посчитать коэффициенты в регрессии $X_i$ на остальные иксы:
\[
X_i = \beta_{i1} X_1 + \beta_{i2} X_2 + \ldots + \beta_{in} X_k + u_i
\]

Оказывается, что
\[
\beta_{it} = -\frac{\mu_{it}}{\mu_{ii}} = - \frac{C^{-1}_{it}}{C^{-1}_{ii}}
\]
Кроме того, дисперсия остатка равна
\[
\Var(u_i) = \frac{1}{\mu_{ii}} = \frac{1}{C^{-1}_{ii}}
\]





Осталось заметить, что
\[
\pCorr(X_i, X_j ; ... ) = -\frac{\Cov(X_i^*, X_j^*)}{\sqrt{\Var(X_i^*)\Var(X_j^*)}} = -\frac{\mu_{ij}}{\sqrt{\mu_{ii}\mu_{jj}}}= -\frac{C^{-1}_{ij}}{\sqrt{C^{-1}_{ii}C^{-1}_{jj}}}
\]

Почему эта формула не срабатывает при $i=j$? При $i=j$ не верной оказывается неожиданная теорема о противоположном знаке частных корреляций.



Подытожим. Мы доказали, что обратная ковариационная матрица имеет следующую структуру:


\[
C^{-1} = \begin{pmatrix}
\frac{1}{\pVar(X_1 ; X_{-1})} & -\frac{\beta_{12}}{\pVar(X_1 ; X_{-1})} & \\
-\frac{\beta_{21}}{\pVar(X_1 ; X_{-1})} & \frac{1}{\pVar(X_2 ; X_{-2})} & \\
 & & \frac{1}{\pVar(X_3 ; X_{-3})} \\
\end{pmatrix},
\]
Где $\beta_{it}$ — коэффициент перед $X_t$ в регрессии $X_i$ на остальные иксы, а $\pVar(X_1 ; X_{-1})$ — дисперсия остатка в той же регрессии.


Вспомним, что наши исходные переменные $\{X_i\}$ и «звезданутые» переменные $\{X_i^*\}$ являются двойственными. По смыслу $X_i^*$ является делённым на свою дисперсию остатком в регрессии $X_i$ на остальные переменные. Значит и $X_i$ является делённым на свою дисперсию остатком в регресии $X_i^*$ на остальные «звезданутые» переменные.



Похожий результат верен и для обратной корреляционной матрицы. Если $A$ — корреляционная матрица, то:

\[
A^{-1} =
\begin{pmatrix}
\frac{\Var(X_1)}{\pVar(X_1 ; X_{-1})} & & \\
 & \frac{\Var(X_2)}{\pVar(X_2 ; X_{-2})} & \\
 & & \frac{\Var(X_3)}{\pVar(X_3 ; X_{-3})} \\
\end{pmatrix}
\]


Частная корреляция между $X_i$ и $X_j$ при фиксированных остальных иксах равна:
\[
\pCorr(X_i, X_j ; \ldots ) = -\frac{A_{ij}}{\sqrt{A_{ii}A_{jj}}}
\]

Стандартизованный коэффициент при $X_j$ в регрессии $X_i$ на остальные иксы равен:
\[
\beta_{ij}^{st} = -\frac{A_{ij}}{A_{ii}}
\]

Квадрат коэффициента множественной корреляции $X_i$ с остальными иксами равен:
\[
\canCorr(X_i; \ldots ) = 1 - \frac{1}{A_{ii}}
\]

\section{Численный пример}

Дана конкретная ковариационная матрица...

\section{Разложение $R^2$ по факторам}

Возьмём $1$-ую строку из матрицы $M$ и помножим на $1$-ый столбец из матрицы $C$.

Получим равенство:
\[
\frac{\Var(X_1)}{\pVar(X_1 ; X_{-1})} - \beta_{12} \frac{\Cov(X_1, X_2)}{\pVar(X_1 ; X_{-1})} - - \beta_{13} \frac{\Cov(X_1, X_3)}{\pVar(X_1 ; X_{-1})} - \ldots = 1
\]

Домножив на общий знаменатель, поделив на $\Var(X_1)$, и, перенеся слагаемые, получаем:
\[
\beta_{12} \frac{\Cov(X_1, X_2)}{\Var(X_1)} + \beta_{13} \frac{\Cov(X_1, X_3)}{\Var(X_1)} + \ldots = 1 - \frac{\pVar(X_1 ; X_{-1})}{\Var(X_1)}
\]

У данной теоретической формулы есть выборочный собрат:

\[
\hat\beta_{12} \frac{\sCov(X_1, X_2)}{\sVar(X_1)} + \hat\beta_{13} \frac{\sCov(X_1, X_3)}{s\Var(X_1)} + \ldots = R^2
\]

Здесь под $R^2$ мы подразумеваем коэффициент детерминации в регрессии переменной $X_1$ на остальные.


\section{Частные корреляции у стационарного процесса}

Произвольная матрица не может быть корреляционной матрицей вектора случайных величин. Она должна быть симметричной и неотрицательно определённой, а на диагонали должны стоять единицы.

Под матрицей частных корреляций можно подразумевать разные вещи.

Вариант А: элемент $(i, j)$ — это частная корреляция между $X_i$ и $X_j$ при фиксированной посторонней величине $Z$.

Здесь неотрицательно определена?

Вариант Б: элемент $(i, j)$ — это частная корреляция между $X_i$ и $X_j$ при фиксированных всех остальных $X_k$.

Здесь $B = 2 I - C$, где $C$ — корреляционная матрица?


У стационарного процесса может быть совершенная любая частная автокорреляционная функция со значениями в диапазоне $(-1;1)$. Например, у стационарного процесса может быть  частная автокорреляционная функция
\[
\phi_{kk} = \begin{cases}
0, \text{ если } k = 1 \\
0.99, \text{ если } k = 2 \\
0, \text{ если } k \geq 3 \\
\end{cases}
\]

А вот и он, $y_t = 0.99 y_{t-2} + \varepsilon_t$:

<<"ar_0_099", message=FALSE>>=
library("forecast")
y <- arima.sim(n = 1000, model = list(ar = c(0, 0.99)))
tsdisplay(y)
@



\section{Почиташки}

\url{https://en.wikipedia.org/wiki/Schur_complement#Applications_to_probability_theory_and_statistics}

\url{https://stats.stackexchange.com/questions/140080/why-does-inversion-of-a-covariance-matrix-yield-partial-correlations-between-ran}

\url{https://stats.stackexchange.com/questions/10795/how-to-interpret-an-inverse-covariance-or-precision-matrix}

\url{http://epublications.bond.edu.au/cgi/viewcontent.cgi?article=1147&context=ejsie}

\url{https://cran.r-project.org/web/packages/corpcor/corpcor.pdf}

\url{http://projecteuclid.org/euclid.aos/1176342881}

\url{http://www.gragusa.org/teaching/et/files/matrix_notes.pdf}


\url{http://www.stata-journal.com/sjpdf.html?articlenum=st0285}


\url{https://www.hse.ru/data/2016/02/15/1140408320/WP2_2016_01_F.pdf}

\url{http://www.ewi.tudelft.nl/fileadmin/Faculteit/EWI/Over_de_faculteit/Afdelingen/Applied_Mathematics/Risico_en_Beslissings_Analyse/Theses/MWawrzyniak_thesis.pdf}

\url{http://www.ewi.tudelft.nl/fileadmin/Faculteit/EWI/Over_de_faculteit/Afdelingen/Applied_Mathematics/Risico_en_Beslissings_Analyse/Theses/MWawrzyniak_thesis.pdf}

\url{http://iroha.scitech.lib.keio.ac.jp:8080/sigma/bitstream/handle/10721/1899/document.pdf?sequence=4}

\section{Выборочные характеристики}


это в статью два?



В теории обычную корреляцию и частную корреляцию можно посчитать, если известен закон распределения случайных величин. На практике закон распределения не известен, однако доступны наблюдения. Как по имеющимся наблюдениям оценить неизвестные корреляции?


Несколько способов оценки корреляции


Здесь пара картинок: википедийная с корреляциями и два ряда случайного блуждания/тренда



Несколько способов оценки частной корреляции


Оценкой для частной дисперсии $X_1$ при фиксированных $X_2$, \ldots, $X_n$ является оценка дисперсии случайной составляющей в регрессии $X_1$ на $X_2$, \ldots, $X_n$


А ещё частную корреляцию можно посчитать в два шага.
Построить регрессию $X_1$ на остальные переменные. Взять $t$-статистику перед $X_2$ и воспользоваться формулой:
\[
\pCorr(X_1, X_2 ; X_{-1, -2}) = \frac{t^2}{t^2 + (n-k)}
\]



Здесь про зависимости возникающие при оценке автокорреляционной и частной автокорреляционной функции для случайных процессов.


Несколько способов оценки множественной корреляции:

...



Обратная к выборочной ковариационной матрице

\[
\hat C^{-1} = \begin{pmatrix}
\frac{1}{\hat \sigma^2_1} & & \\
 & \frac{1}{\hat \sigma^2_2} & \\
 & & \frac{1}{\hat \sigma^2_3} \\
\end{pmatrix}
\]

Если мы предположим, что разные наблюдения в выборке независимы и одинаково распределены, то получим такие выборочные оценки

\begin{tabular}{lll} \toprule
Теоретический & Выборочный & Коммент \\
\midrule
$\pVar(y_i ; z_i)$      & $\hat \sigma_u^2$ в регрессии $y$ на $z$, выборочная дисперсия остатков & несмещённость? \\
$\pCov(y_i, x_i ; z_i)$ & выборочная ковариация остатков двух регрессий & несмещённость \\
$\canCorr(y_i; x_{i1}, x_{i2}, \ldots, x_{ik})$ & $R^2$, $R^2_{adj}$ &  \\
коэффициент в множественной регрессии & $\hb = (X'X)^{-1}X'y$ & несмещенный \\
стандартизированный коэффициент в множественной регрессии & $\hb_j^{st}=\hb_j \frac{\hat\sigma_x}{\hat\sigma_y}$, $\hb^{st} = (X^{st\prime}X^{st})^{-1}X^{st\prime}y^{st}$ & несмещенный? \\
обычная корреляция & $sCorr(y, x) = \frac{\sum (x_i - \bar x)(y_i - \bar y)}{\sqrt{...}}$ & \\
частная корреляция & корреляция остатков регрессии, обращение корреляционной матрицы & \\
\bottomrule
\end{tabular}

\section{Дисперсия МНК-оценок}

Обратные матрицы окружают нас, они повсюду! Как известно, в модели множественной регрессии

\[
\Var(\hb |X) = \sigma^2 (X'X)^{-1}
\]

На диагонали этой матрицы стоят дисперсии отдельных $\hb_j$. И их можно получить в явном виде! Снова нам на помощь придёт матрица-мать всех регрессий!

Чтобы понять, что у неё на диагонали применим аналогичный трюк. Пусть $x_i$ — это столбцы матрицы $X$. От векторов $x_i$ перейдём к векторам $x_i^*$. По своей сути $x_i^*$ будeт пропорционален остатку в регрессии $x_i$ на остальные $x_j$. Строить величину $x_i^*$ мы будем как линейную комбинацию $x_1$, $x_2$, \ldots, $x_k$ с некими весами $\mu_{ij}$,
\[
x_i^* = \sum_{t=1}^k \mu_{it} x_t
\]

Коэффициенты $\mu_{it}$ должны быть такими, чтобы выполнялось условие:
\[
\langle x_i^*, x_j \rangle = \begin{cases}
1, \text{ при } i = j \\
0, \text{ при } i\neq j \\
\end{cases}
\]

Первый смысл коэффициента $\mu_{it}$ — вес, с которым вектор $x_t$ входит в вектор $x_i^*$. Заметим, что есть и другая интерпретация, а именно, эти коэффициенты показывают скалярное произведение между новыми $x_i^*$:

\[
\langle x_i^*, x_j^* \rangle= \langle x_i^*, \sum_{k} \mu_{jk} x_k \rangle = \mu_{ji}
\]

Из этого, в частности, следует, что $\mu_{ij}=\mu_{ji}=\langle x_i^*, x_j^* \rangle$ и $\mu_{ii} = |x_i^*|^2$.


Из условия:
\[
x_i^* = \sum_{t=1}^k \mu_{ik} x_t
\]

Следует также представление:
\[
x_i = \sum_{t \neq i} - \frac{\mu_{it}}{\mu_{ii}}x_t  + \frac{1}{\mu_{ii}}x_i^*
\]

То есть $\frac{1}{\mu_{ii}}x_i^*$ — это остаток от регрессии $x_i$ на остальные $x_j$. Заметим попутно, что квадрат длины этого остатка, или $RSS_i$, равен:
\[
\left| \frac{1}{\mu_{ii}}x_i^* \right|^2 = \frac{\mu_{ii}}{\mu_{ii}^2} = \frac{1}{\mu_{ii}}
\]


Выпишем, как связаны коэффициенты $\mu_{ij}$ со скалярными произведениями между $x_i$:
\begin{equation}
\langle x_i^*, x_j \rangle = \sum_{t=1}^n \mu_{it}\langle x_t, x_j \rangle = \begin{cases}
1, \text{ при } i = j \\
0, \text{ при } i\neq j \\
\end{cases}
\end{equation}


В матричном виде это же соотношение записывается потрясающе просто:
\[
M\cdot C = I
\]
То есть $M=C^{-1}$. Таким образом получается, что обратная ковариационная матрица позволяет легко посчитать коэффициенты любой регрессии!

В частности, на диагонали у неё находятся числа $\mu_{ii}=1/RSS_i$.

\section{Геометрия обратной матрицы}


Если:
\begin{enumerate}
\item $r_1$, $r_2$, \ldots, $r_n$ — это строки матрицы $A$,
\item $c_1^{-1}$, $c_2^{-1}$, \ldots, $c_n^{-1}$ — это столбцы матрицы $A^{-1}$, \item $V_i$ — это гипер-плоскость, линейная оболочка всех столбцов обратной матрицы кроме столбца $c_i^{-1}$, то есть $V_i = \Lin(c_1^{-1}, \ldots, c_{i-1}^{-1}, c_{i+1}^{-1}, \ldots, c_n^{-1})$
\end{enumerate}
то:

\begin{enumerate}
\item Cтрока $r_i$ перпендикулярна столбцу $c_j^{-1}$ при $j\neq i$, $\langle r_i, c_j^{-1} \rangle =0$
\item Cкалярное произведение $r_i$ и $c_i^{-1}$ равно единице, $\langle r_i, c_i^{-1} \rangle =1$
\item $\text{length}(r_i)\cdot \text{distance}(c_i^{-1}, V_i) = 1$.

Действительно, спроецируем столбец $c_i^{-1}$ на $V_i$, получим вектор $\hat c_i^{-1}$. Получаем цепочку,
\[
1 = \langle r_i, c_i^{-1} \rangle = \langle r_i, \hat c_i^{-1} + (c_i^{-1} - \hat c_i^{-1}) \rangle
\]

Заметим, что и вектор $r_i$, и вектор $(c_i^{-1} - \hat c_i^{-1})$, перпендикулярны гипер-плоскости $V_i$. Поэтому угол между ними равен нулю. И, следовательно,

\[
\langle r_i, \hat c_i^{-1} + (c_i^{-1} - \hat c_i^{-1}) \rangle =  \langle r_i,  (c_i^{-1} - \hat c_i^{-1}) \rangle = |r_i| \cdot |c_i^{-1} - \hat c_i^{-1}| \cdot \cos(0) = |r_i| \cdot |c_i^{-1} - \hat c_i^{-1}|
\]

Для симметричной матрицы аналогичное утверждение верно про строки исходной матрицы и строки обратной.

\url{http://mathoverflow.net/questions/120884/}

Ещё один геометрический факт:

Если каждую строку $r_i$ интепретировать как точку в $\RR^n$, то через эти $n$ точек можно провести гипер-плоскость $\alpha$ (она вовсе не обязательно содержит начало координат, не стоит путать её с $\Lin(r_1, r_2, \ldots, r_n))$.

Обозначим также $\II$ — вектор столбец из единиц. Тогда вектор построчных сумм $A^{-1}\cdot \II$ перпендикулярен гипер-плоскости $\alpha$.

Как это использовать в метрике?

\end{enumerate}



\section{До кучи}

Есть малоизвестная теорема Кроба:
\begin{theorem}
Рациональное равенство является тождеством для произвольного кольца, если и только если его можно доказать дополнительно используя бессмысленную в кольце формулу суммы бесконечно убывающей геометрической прогрессии $(1-a)^2 = 1 + a + a^2 + a^3 + \ldots$.
\end{theorem}

\url{http://mathoverflow.net/questions/31595}

Эта теорема позволяет легко и строго выводить многие матричные тождества:

Например. Докажите, что
\[
(I - BA)^{-1} = I + B(I- AB)^{-1}A
\]

Решение. Применим разрешённую Кробом формулу бесконечно убывающей геометрической прогрессии два раза:
\begin{multline*}
(I - BA)^{-1} = I + BA + BABA + BABABA + \ldots = \\
= I + B(I + AB + ABAB + \ldots)A=I + B(I- AB)^{-1}A
\end{multline*}

\section{Упражнения}

\begin{enumerate}
\item Докажите, что
\[
(I+A)(I-BA)^{-1}(I+B) =  (I+B)(I-AA)^{-1}(I+A)
\]



\end{enumerate}



\end{document}
