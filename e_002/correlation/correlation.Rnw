\documentclass[10pt]{article}

\usepackage{epsilonj}

\usepackage{tikz}
\usetikzlibrary{calc,intersections,decorations.pathreplacing}
\usetikzlibrary{arrows.meta}
% этот код считает отмечает углы в tikz
\newcommand\markangle[6]{% origin X Y radius radiusmark mark
  % fill red circle
  \begin{scope}
    \path[clip] (#1) -- (#2) -- (#3);
    \fill[color=red,fill opacity=0.5,draw=red,name path=circle]
    (#1) circle (#4);
  \end{scope}
  % middle calculation
  \path[name path=line one] (#1) -- (#2);
  \path[name path=line two] (#1) -- (#3);
  \path[%
  name intersections={of=line one and circle, by={inter one}},
  name intersections={of=line two and circle, by={inter two}}
  ] (inter one) -- (inter two) coordinate[pos=.5] (middle);
  % put mark
  \node at ($(#1)!#5!(middle)$) {#6};
}

\RequirePackage{graphicx}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\RR}{\mathbb{R}}
\def \hb{\hat{\beta}}
\DeclareMathOperator{\pCorr}{pCorr}
\DeclareMathOperator{\canCorr}{canCorr}
\DeclareMathOperator{\pCov}{pCov}
\DeclareMathOperator{\pVar}{pVar}
\DeclareMathOperator{\Dep}{Dep}
\renewcommand{\P}{\mathbb{P}}


\begin{document}

\TITLE{Матрица-мать всех регрессий}
\SHORTTITLE{Матрица-мать всех регрессий}

\AUTHOR{Борис Демешев}{НИУ ВШЭ, Москва.}
\SHORTAUTHOR{Борис Демешев}

\DoFirstPageTechnicalStuff


\newtheorem{theorem}{Теорема}
\newtheorem{definition}{Определение}


\begin{abstract}
Корреляция — это способ описать силу линейной зависимости между двумя случайными величинами одним числом. Каков геометрический смысл корреляции? Что такое частная корреляция? Как связаны частная и условная корреляция? Как связаны частные корреляции, коэффициенты регрессии и стандартизованные коэффициенты регрессии? И, главное, есть матрица-мать, порождающая все эти меры зависимости.
\end{abstract}

\begin{keyword}
корреляция, частная корреляция, условная корреляция, косинус, проекция, стандартизованный коэффициент, теорема Фриша-Вау-Ловелла, матрица-мать всех регрессий
\end{keyword}


\section{Сколько вешать в граммах?}

% Люди часто хотят найти простое решение сложной задачи.

Everything Should Be Made as Simple as Possible, But Not Simpler. Roger Sessions, paraphrase of Albert Einstein. 

There is always a well-known solution to every human problem — neat, plausible, and wrong. Henry Louis Mencken.


Почему при болезни мы измеряем температуру тела? По всей видимости важны два факта:

\begin{enumerate}
\item Измерить температуру очень удобно
\item Это измерение несёт в себе информацию о здоровье
\end{enumerate}

Сложное описание состояния здоровья сводится измерением температуры к одной цифре. Естественно, куча информации теряется в этой цифре и бессмысленно лечить человека, руководствуясь только температурой его тела. Температура $40^{\circ}$ говорит, что что-то не так, но что — непонятно. А температура $36.6^{\circ}$ ещё не говорит о том, что у человека идеальное здоровье. Однако процедура очень проста и в некоторых ситуациях, например, при обыкновенной простуде, её достаточно для принятия решения о приёме жаропонижающего. 

Если бы для измерения температуры нужно было специальное устройство размером с половину комнаты, никто бы дома её не мерял. Люди любят упрощать, а здесь простота измерения очень важна!

Подобном образом дела обстоят и с описанием зависимости между случайными величинами. Зависимость между случайными величинами полностью описывается их совместной функцией распределения, $F(x,y)=\P(X\leq x, Y\leq y)$. Это слишком сложно! Вместо сложной функции распределения мы хотим получить одно число. Некую «силу зависимости». Назовём это мифическое число $\Dep(X,Y)$.

Чего бы нам хотелось от этого числа?
\begin{enumerate}
\item Посчитать это число очень удобно
\item Это число несёт в себе информацию о зависимости
\end{enumerate}

В случае двух случайных величин наиболее популярными мерами зависимости являются ковариация, корреляция и коэффициент в парной регрессии.

\section{Ковариация}

В каком смысле «удобно» считать?

Очень часто возникают суммы случайных величин, поэтому было бы здорово, чтобы у суммы легко считалась наша характеристика $\Dep(X+Z,Y)$. Проще всего было бы, если бы:

\[
\Dep(X+Z,Y)=\Dep(X,Y)+\Dep(Z,Y)
\]

И, конечно, мы ждём, что у независимых случайных величин нулевая сила зависимости, $\Dep(X,Y)=0$, а ненулевая сила зависимости, $\Dep(X,Y)\neq 0$, была бы возможна только у зависимых случайных величин.

Этим двум требуемым свойствам, простота подсчёта и информация о зависимости, отвечает ковариация.

\begin{definition}
Ковариация величин $X$ и $Y$ равна
\[
\Cov(X,Y)=\E\left[  (X-\E(X)) (Y-\E(Y))   \right] = \E(XY)-\E(X)\E(Y)
\]
\end{definition}

здесь рассказать про прямоугольники и площадь с плюсом/минусом?




\section{Коэффициенты в парной регрессии}

Наиболее легко интерпретировать коэффициенты в парной регрессии. Пусть у нас есть две случайных величины $X_1$ и $X_2$. Мы можем получить два коэффициента:

\begin{itemize}
\item коэффициент $\beta_{12}$ — на сколько в среднем растёт $X_1$ при росте $X_2$ на единицу
\item коэффициент $\beta_{21}$ — на сколько в среднем растёт $X_2$ при росте $X_1$ на единицу
\end{itemize}


А теперь из этого словесного определения мы получим формулу. Разложим величину $X_1$ на два слагаемых. Первое слагаемое вбирает в себя всю ту часть $X_1$, которая линейно зависит от $X_2$, а второе — всё оставшееся:

% \[
% \frac{Y}{\sigma_Y}=\rho \cdot \frac{X}{\sigma_X} + \varepsilon
% \]
% 
% В этой формуле видно, что с ростом $X$ на одно стандартное отклонений $\sigma_X$ правая часть изменится в среднем на $\rho$, и, следовательно, величина $Y$ в среднем изменится на $\rho \cdot \sigma_Y$.

\[
X_1=\beta_{12} \cdot X_2 + u
\]

Здесь $\beta_{12}$ показывает на сколько единиц в среднем растёт $X_1$ при росте $X_2$ на одну единицу.

Естественно, мы хотим, чтобы с ростом $X_2$ величина $\varepsilon$ в среднем не менялась, то есть хотим нулевую «силу зависимости» между ними, $\Cov(X_2, u)=0$.


\[
\Cov\left(X_2, X_1 - \beta_{12} X_2 \right) = 0
\]

По свойствам ковариации получаем

\[
\Cov(X,Y)/\sigma_Y- \rho \Cov(X,X)/\sigma_X=0
\]

И, тадам, выражаем коэффициент $\beta_{12}$:

\[
\beta_{12} = \frac{\Cov(X_1,X_2)}{\Var(X_2)}
\]


Заметим, что хотя знак $\beta_{12}$ и знак $\beta_{21}$ совпадают, сами они будут совпадать только если $\Var(X_1) = \Var(X_2)$.


Коэффициент $\beta_{12}$, естественно, будет реагировать на изменение единиц измерения $X_1$ или $X_2$. Если увеличить $X_1$ в 10 раз, то и $\beta_{12}$ увеличиться в 10 раз. Иногда чувствительность к единицам измерения является нежелательной. 

Коэффициент регрессии можно избавать от чувствительности к единицам измерения, если стандартизировать исходные переменные, то есть поделить их на стандартные отклонения. При этом мы получим стандартизированный коэффициент регрессии:

\[
\beta_{12}^{st} = \frac{\Cov(X_1/\sigma_1, X_2/\sigma_2)}{\Var(X_2/\sigma_2)} = \frac{\Cov(X_1,X_2)}{\sqrt{\Var(X_1)\Var(X_2)}} = \beta_{12} \frac{\sigma_2}{\sigma_1}
\]

По своему смыслу $\beta_{12}^{st}$ показывает, на сколько своих стандартных отклонений в среднем изменяется $X_1$ при изменении $X_2$ на одно своё стандартное отклонение. 

Заметим, что произошло чудо! В силу симметричности выражения для $\beta_{12}^{st}$ оказывается, что $\beta_{12}^{st}=\beta_{21}^{st}$. Получается, с одной стороны, что:
\[
\frac{X_1}{\sigma_1} = \beta_{st}\frac{X_2}{\sigma_2} + u_1
\]
А с другой стороны, в противоположном разложении будет тот же коэффициент:
\[
\frac{X_2}{\sigma_2} = \beta_{st}\frac{X_1}{\sigma_1} + u_2
\]


Это чудо слегка контр-интуитивно! Если бы зависимость между $X_1$ и $X_2$ была бы жёстко детерминистически линейной, и с ростом $X_2$ на единицу величина $X_1$ росла бы на $\Delta$, то с ростом $X_1$ на единицу величина $X_2$ росла бы на $1/\Delta$. Для случайных величин обращения не происходит! Если с ростом $X_1$ на одно своё стандартное отклонение величина $X_2$ в среднем растёт на $\Delta$ своих стандартных отклонений, то и с ростом $X_2$ на одно своё стандартное отклонение величина $X_1$ в среднем растёт на $\Delta$ своих стандартных отклонений.

Здесь задачи от Фрэнсиса Галтона?

\section{Корреляция по-русски}

Обычно в учебниках даётся такое определение корреляции

\begin{equation}
\label{def_corr}
\Corr(X,Y)=\frac{\Cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}.
\end{equation}

Естественно, возникает вопрос: «С какого перепугу? Почему это мы делим ковариацию на что-то там?»

Мы дадим естественное определение корреляции:

\begin{definition}
Корреляция величин $X_1$ и $X_2$ — это среднее геометрическое среднего роста $X_1$ при изменении $X_2$ на еденицу и среднего изменения $X_2$ при росте $X_1$ на единицу, совпадающее знаком с этими изменениями:

\[
\Corr(X, Y) = \sqrt{\beta_{XY}\beta_{YX}} \cdot \sign(\beta_{XY})
\]

\end{definition}

Эти два определения полностью эквивалентны, действительно:
\[
\Corr(X, Y)^2 = \frac{\Cov(X,Y)^2}{\Var(X)\Var(Y)} = \beta_{12} \cdot \beta_{21}
\]

Посмотрим, как коэффициент корреляции связан со стандартизованными коэффициентам регрессии:
\[
\beta_{12} \cdot \beta_{21} = \beta_{12}^{st} \cdot \beta_{21}^{st}= (\beta_{12}^{st})^2 = (\beta_{21}^{st})^2
\]

То есть коэффициент корреляции просто равен стандартизованному коэффициенту регрессии! Поэтому корреляция между величинами $X$ и $Y$ также показывает, на сколько своих стандартных отклонений в среднем изменяется $X$ при изменении $Y$ на одно своё стандартное отклонение. 


\section{Геометрический смысл корреляции}

Давайте рисовать случаные величины векторами-стрелочками! Не в том смысле, что у стрелочки случайное направление или длина, а в том смысле, что направление и длина стрелочки описывают характеристики этой случайной величины.

Любую геометрию можно задать, задав скалярное произведение. Действительно, если мы умеем считать скалярное произведение двух любых векторов, $\langle\vec{a},\vec{b}\rangle$, то длина вектора считается ровно как в 9-м классе:

\[
|\vec{a}|=\sqrt{\langle\vec{a},\vec{a}\rangle}
\]

И также любой девятикласник помнит, что косинус угла между векторами считается как

\[
\cos(\vec{a},\vec{b})=\frac{\langle\vec{a},\vec{b}\rangle}{|\vec{a}||\vec{b}|}
\]

Мы определим скалярное произведение двух случайных величин как их ковариацию:

\[
\langle X,Y\rangle=\Cov(X,Y)
\]

При таком подходе длиной случайной величины окажется стандартное отклонение:

\[
\sqrt{\Cov(X,X)}=\sqrt{\Var(X)}=\sigma_X
\]

А корреляция окажется косинусом угла между случайными величинами:
\[
\cos \phi =\cos(X,Y)=\frac{\Cov(X,Y)}{\sigma_X\sigma_Y}=\Corr(X,Y)
\]

\begin{center}
\begin{tikzpicture}
\path (0,0) coordinate (origin);
\path (origin) ++(1,2) coordinate (Y);
\path (origin) ++(3,0) coordinate (X);
\draw[-{Latex[length=3mm]},thick] (origin) -- (Y);
\draw[-{Latex[length=3mm]},thick] (origin) -- (X);
\node [above] at (X) {$X$};
\node [right] at (Y) {$Y$};
\markangle{origin}{X}{Y}{6mm}{4mm}{$\phi$}
\end{tikzpicture}
\end{center}

Значит в нашей геометрии  длина стрелочки — стандартное отклонение случайной величины, а косинус угла между двумя стрелочками — это корреляция двух случайных величин. Дисперсия, следовательно, это квадрат длины случайной величины. Перпендикулярными случайными величинами будут те, косинус угла между которыми равен нулю, то есть некоррелированные. Автоматически оказывается, что корреляция лежит в диапазоне от $(-1)$ до $1$.

Например, сформулируем в данной геометрии теорему Пифагора. Если случайные величины $X$ и $Y$ перпендикулярны (корреляция или ковариация равны нулю), то дисперсия их суммы (квадрат длины гипотенузы) равен сумме их дисперсий (сумму квадратов длин катетов):

\[
\Var(X+Y)=\sigma^2_{X+Y}=\sigma^2_X+\sigma^2_Y=\Var(X)+\Var(Y)
\]

\begin{center}
\begin{tikzpicture}
\path (0,0) coordinate (origin);
\path (origin) ++(4,2) coordinate (Y);
\path (origin) ++(4,0) coordinate (X);
\draw[-{Latex[length=3mm]},thick] (origin) -- (Y);
\draw[-{Latex[length=3mm]},thick] (origin) -- (X);
\draw[-{Latex[length=3mm]},thick] (X) -- (Y);
\node [yshift=10pt,xshift=-15pt] at (X) {$X$};
\node [yshift=-15pt,xshift=15pt] at (Y) {$Y$};
\node [yshift=5pt,xshift=-30pt] at (Y) {$X+Y$};
\draw [decorate,decoration={brace,amplitude=10pt}] (X) -- (origin) node [black,midway,yshift=-15pt] {\footnotesize $\sigma_X$};
\draw [decorate,decoration={brace,amplitude=10pt}] (Y) -- (X) node [black,midway,xshift=20pt] {\footnotesize $\sigma_{Y}$};
\draw [decorate,decoration={brace,amplitude=10pt}] (origin) -- (Y) node [black,midway,yshift=15pt,xshift=-10pt] {\footnotesize $\sigma_{X+Y}$};
\end{tikzpicture}
\end{center}

Введение геометрии позволяет говорить о проекции. Например, можно спроецировать случайную величину $Y$ на множество случайных величин пропорциональных величине $X$, $\{cX | c\in \RR \}$. Если на обычной плоскости спроецировать вектор $\vec{a}$ на прямую, порожденную вектором $\vec{b}$, то получится $\cos(\vec{a},\vec{b})\cdot \vec{b}$. По аналогии, если спроецировать случайную величину $Y$ на множество $\{cX | c\in \RR \}$, то получится $\Corr(X,Y)\cdot X$. Другими словами, среди случайных величин пропорциональных $X$ величина  $\hat{Y}=\Corr(X,Y)\cdot X$ — самая похожая на величину $Y$.

Понятие проекции позволяет интепретировать квадрат корреляции. Квадрат косинуса равен отношению квадрата длины прилежащего катета $\Var(\hat{Y})$ к квадрату гипотенузы $\Var(Y)$.

(картинка)

Следовательно, $\Corr(X,Y)^2=\frac{\Var(\hat{Y})}{\Var(Y)}$, то есть квадрат корреляции показывает долю дисперсии $Y$, которую можно объяснить с помощью величин пропорциональных $X$.

\section{Корреляция и независимость}

\begin{theorem}
Случайные величины $X$ и $Y$ независимы тогда и только тогда, когда некоррелированы любые функции $f(X)$ и $g(Y)$.
\end{theorem}

Другими словами для независимости $X$ и $Y$ необходима некоррелированность пар $X$ и $Y$, $X^2$ и $\cos(Y)$, $\exp(X)$ и $1/Y$, и так далее. Из этого следует, что некоррелированность $X$ и $Y$ является необходимым, но недостаточным условием для независимости.

Можно выделить три «степени» независимости случайных величин $X$ и $Y$:

\begin{tabular}{rl}
\toprule
Некоррелированность $Y$ и $X$ & $\Corr(X,Y)=0$ \\
$\E(Y|X)=\E(Y)$ & $\Corr(f(X),Y)=0$ для всех $f()$ \\
Независимость $Y$ и $X$ & $\Corr(f(X),g(Y))=0$ для всех $f()$ и $g()$ \\
\bottomrule
\end{tabular}



Многие ошибочно считают, что если величина $X$ имеет нормальное распределение $N(\mu_X, \sigma^2_X)$ и величина $Y$ имеет нормальное распределение $N(\mu_Y, \sigma^2_Y)$, и $X$ и $Y$ некоррелированы, то они независимы. Это неверно.

Контрпример. Случайная величина $X$ имеет стандартное нормальное распределение $N(0;1)$, случайная величина $Z$ независима от $X$ и равновероятно принимает значения $-1$ и $+1$. Определим величину $Y$ как их произведение, $Y=XZ$.

В этом примере величины $X$ и $Y$ зависимы, так как $|X|=|Y|$. Однако $Y$ распределена нормально стандартно и $\Corr(X,Y)=0$.

Правильная теорема звучит так:

\begin{theorem}
Если некоррелированные случайные величины $X$ и $Y$ имеют совместное нормальное распределение, то $X$ и $Y$ независимы.
\end{theorem}

Попутно упомянем ещё одно неожиданное свойство предъявленного контрпримера. Если случайные величины нормальны по отдельности, то вполне возможно, что их сумма ненормальна. Для пары величин, имеющих совместное нормальное распределение, это невозможно.



\section{Более двух случайных величин}

Если случайных оказывается больше двух, то для описания зависимости с помощью одного числа оказывается очень много вариантов. Среди популярных можно назвать:
\begin{enumerate}
\item коэффициент бета в множественной регрессии
\item стандартизированный коэффициент множественной регрессии
\item частная корреляция
\item каноническая корреляция
\end{enumerate}


\section{Стандартизованный коэффициент регрессии}

Для нахождения стандартизованного коэффициента множественной регрессии используется разложение
\[
\frac{Y}{\sigma_Y}=\rho_{XY|Z} \cdot \frac{X}{\sigma_X} + \rho_{YZ|X} \frac{Z}{\sigma_Z} + \varepsilon
\]

Два подхода к определению стандартизованного коэффициента регрессии эквивалентны в силу теоремы Фриша-Ву-Ловелла (Frisch–Waugh–Lovell). Обычно эта теорема формулируется применительно к регрессии, а здесь мы приведём её вариат для случайных величин.

\begin{theorem}
Если имеют место разложения:
\[
Y=a_1 Z_1 + a_2 Z_2 + \ldots + a_n Z_n + \tilde{Y}, \text{ где }\; \tilde{Y}\bot Z_1, Z_2, \ldots, Z_n
\]
и
\[
X=b_1 Z_1 + b_2 Z_2 + \ldots + b_n Z_n  + \tilde{X}, \text{ где }\; \tilde{X}\bot Z_1, Z_2, \ldots, Z_n
\]
То в разложениях
\[
\tilde{Y}=d \tilde{X} + \varepsilon, \text{ где }\; \varepsilon \bot \tilde{X}
\]
и
\[
Y=c_1 Z_1 + c_2 Z_2 + \ldots + c_n Z_n  + dX + u, \text{ где }\; u \bot Z_1, Z_2, \ldots, Z_n, X
\]
коэффициенты при $\tilde{X}$ и $X$ совпадают.
\end{theorem}


\section{Частная корреляция}

Есть два словесных определения частной корреляции. Первое — через коэффициенты в регрессиях:

\begin{definition}
Частная корреляция между величинами $X_1$ и $X_2$ при фиксированной величине $X_3$ — это среднее геометрическое между коэффициентом при $X_2$ в регрессии $X_1$ на остальные переменные и коэффициентом при $X_1$ в регрессии $X_2$ на остальные переменны, совпадающее знаком с этими коэффициентами
\end{definition}

Попутно первое определение использует нетривиальных факт, что знак при $X_1$ в регрессии $X_2$ на остальные переменные и знак при $X_2$ в регрессии $X_1$ на остальные переменные совпадают.

Второе определение — через остатки и обычную корреляцию:

\begin{definition}
Частная корреляция между величинами $X_1$ и $X_2$ при фиксированной величине $X_3$ — это есть обычная корреляция между остатками регрессии $X_1$ на $X_3$ и остатками регрессии $X_2$ на $X_3$.
\end{definition}

Согласно второму определению корреляция показывает на сколько своих стандартных отклонений в среднем вырастет величина $Y$, очищенная от связи с $Z$, при росте величины $X$, очищенной от связи с $Z$, на одно своё стандартное отклонение.


Подход к подсчёту частной корреляции следующий:
\begin{enumerate}
\item Спроецируем $X$ на множество величин, некоррелированных с $Z$. Получим $\tilde{X}$.
\item Спроецируем $Y$ на множество величин, некоррелированных с $Z$. Получим $\tilde{Y}$.
\item Частная корреляция между $X$ и $Y$ при фиксированной $Z$ — это обычная корреляция между $\tilde{X}$ и $\tilde{Y}$.
\end{enumerate}


(картинка ...)


доказательство эквивалентности подоходов?

\section{Частная дисперсия и ковариация}

Определения частной корреляции можно сформулировать с помощью частной ковариации и частной дисперсии.

\[
\pCorr(X, Y|Z) = \frac{\pCov(X,Y|Z)}{\sqrt{\pVar(X|Z)\pVar(Y|Z)}}
\]

Частная дисперсия величины $X$ очищенной от $Z$ определяется как

\[
\pVar(X|Z) = \Var(X-aZ), \text{ где  константа } a \text{ такова, что} \Cov(X-cZ, Z) = 0
\]

Частная ковариация

\[
\pCov(X,Y|Z) = \Cov(X-aZ, Y- bZ ), \text{ где  константы } a \text{ и } b \text{ такова, что} \Cov(X-aZ, Z) = \Cov(Y-bZ, Z) = 0,
\]


\section{Условная корреляция}

\begin{definition}
Условная корреляция между величинами $X$ и $Y$ при известном значении величины $Z$ показывает на сколько своих стандартных отклонений $\sigma_Y$ в среднем вырастет $Y$ при росте величины $X$ на одно своё стандартное отклонение $\sigma_X$ при заданном значении величины $Z$.
\end{definition}


Следует подчеркнуть одно существенное отличие условной корреляции от обычной и частной. Обычная и частная корреляция являеются константами. Условная корреляция $\Corr(X,Y|Z)$ является функцией от $Z$. Величина $Z$ является случайной, поэтому и условная корреляция $\Corr(X,Y|Z)$ является случайной величиной.

Здесь регрессионное определение????

Чуть более формальное определение:

\begin{definition}
\[
\Corr(X,Y|Z) = \frac{\Cov(X,Y|Z)}{\sqrt{\Var(X|Z)\Var(Y|Z)}},
\]
\end{definition}

где $\Cov(X,Y|Z)=\E(XY|Z)-\E(X|Z)\E(Y|Z)$ и $\Var(X|Z)=\E(X^2|Z)-(\E(X|Z))^2$


Пример подсчета частной и условной корреляций.

Пример 1.

Закон распределения случайных величин $X_1$, $X_2$, $X_3$ задан двумя таблицами:

\begin{center}
\begin{tabular}{cc}
\begin{tabular}{ccc}
& $X_3=0$ &    \\
& & \\
\cline{1-3}
 & $X_2=0$ & $X_2=1$ \\
\cline{1-3}
$X_1=0$ & $0.06$ & $0.08$ \\
$X_1=1$ & $0.24$ & $0.32$  \\
\end{tabular}
&
\begin{tabular}{ccc}
& $X_3=1$ &    \\
& & \\
\cline{1-3}
 & $X_2=0$ & $X_2=1$ \\
\cline{1-3}
$X_1=0$ & $0.1$ & $0$ \\
$X_1=1$ & $0$ & $0.2$ \\
\end{tabular}
\end{tabular}
\end{center}

Найдите условную корреляцию $\Corr(X_1,X_2|X_3)$ и частную корреляцию $\pCorr_{X_3}(X_1,X_2)$.

Решение.

Эти две таблички на самом деле реализуют простую мысль: при $X_3=1$ величины $X_1$ и $X_2$  связаны детерминистически линейно, а при $X_3=0$ величины $X_1$ и $X_2$ независимы.

Считаем две вспомогательные условные корреляции, $\Corr(X_1,X_2|X_3=0)=0$, $\Corr(X_1,X_2|X_3=1)=1$.

Отсюда получаем, что $\Corr(X_1,X_2|X_3)=X_3$. Для дискретных случайных величин запись условного ожидания не однозначна, и, например, ответ $\Corr(X_1,X_2|X_3)=X_3^2$ также будет верным.



Пример 2.

Величины $X_1$, $X_2$, $X_3$ имеют совместное нормальное распредлеение с математическим ожиданием $\E(X)=(1,2,-3)'$ и ковариационной матрицей
\[
\Var(X)=\begin{pmatrix}
9 & 2 & -1 \\
2 & 16 & 1 \\
-1 & 1 & 4
\end{pmatrix}
\]
Найдите условную корреляцию $\Corr(X_1,X_2|X_3)$ и частную корреляцию $\pCorr_{X_3}(X_1,X_2)$.

Решение.

\ldots

В данном примере частная и условная корреляция совпали. Это одно из приятных свойств многомерного нормального распределения:


\begin{theorem}
Если величины $X$, $Y$ и $Z$ имеют совместное нормальное распределение, то:
\begin{enumerate}
\item частная и условная дисперсии совпадают
\item частная и условная ковариации совпадают
\item частная и условная корреляции совпадают
\end{enumerate}
\end{theorem}


Прикольный пример. $X \sim U[0;1]$. При фиксированном $X$ величины $Y$ и $Z$ условно независимы и одинаково распределены $Y|X \sim U[0; X]$ и $Z|X \sim U[0;X]$. В данном случае условная корреляция $\Corr(Y,Z | X) = 0$, однако $\pCorr(Y, Z | X) \neq 0$. Или $[0;X^2]$?


\section{Случай равных дисперсий}



Если дисперсии двух рассматриваемых величин $X_1$ и $X_2$ равны, то стандартизированный коэффициент регрессии равен обычному коэффициенту регрессии, $\beta_{12}=\beta_{12}^{st}$.

Если же выполнено более сильное условие стационарности случайного процесс $X_1$, $X_2$, \ldots, то 
\[
\pCorr(X_1, X_k | X_{2,3, \ldots, (k-1)}) = \beta_{1k} = \beta_{k1} = \beta_{1k}^{st} = \beta_{k1}^{st}
\]

К данному случаю относиться частная автокорреляционная функция для стационарных процессов:

Пример 3. AR(1) процесс

\[
y_t = 0.7 y_{t-1} + u_t
\]

Найдите $\pCorr(y_t, y_{t-3} | y_{t-1}, y_{t-2})$.

Частная корреляция в данном случае обычному коэффициенту регрессии, то есть коэффициенту перед $y_{t-3}$ в регрессии $y_{t}$. Замечаем, что
\[
y_t = 0.7 y_{t-1} + 0 y_{t-2} + 0 y_{t-3} + u_t
\]

Значит,  $\pCorr(y_t, y_{t-3} | y_{t-1}, y_{t-2}) = 0$.

\section{Каноническая корреляция}

Если разбить все случайные величины на две группы, скажем на иксы и игреки, то каноническая корреляция — это максимальная корреляция между линейной комбинацией иксов и линейной комбинацией игреков.

\[
\canCorr(Y_1, Y_2, Y_3; X_1, X_2) = \max_{\alpha_1, \alpha_2, \beta_1, \beta_2, \beta_3} \Corr(\alpha_1 X_1 + \alpha_2 X_2, \beta_1 Y_1 + \beta_2 Y_2 +\beta_3 Y_3)
\]

Если группа игреков состоит из одной случайной величины, то мы получаем коэффициент множественной корреляции, квадрат которого можно назвать «теоретическим $R^2$».

\[
\canCorr(Y; X_1, X_2, \ldots, X_k)
\]


И Шахерезаду застигло утро, и она прекратила дозволенные речи, заметив, что вышла замечательная статья Димы Борзых о методе канонических корреляций.


\section{Матрица — Мать всех регрессий и корреляций}

Оказывается многие упомянутые меры зависимости порождаются обратной ковариационной матрицей. Пусть $X$ — это вектор случайных величин $X=(X_1, X_2, \ldots, X_k)'$ с ковариационной матрицей $C=\Var(X)$.


От величин $X_1$, $X_2$, \ldots, $X_n$ мы перейдём к величинам $X_1^*$, $X_2^*$, \ldots, $X_n^*$. По своей сути $X_i^*$ будет пропорциональна той части $X_i$, которая не коррелирована ни с одним оставшимся $X_j$. Строить величину $X_i^*$ мы будем как линейную комбинацию $X_1$, $X_2$, \ldots, $X_n$ с некими весами $\beta_{ij}$, 
\[
X_i^* = \sum_{k=1}^n \beta_{ik} X_k
\]

Коэффициенты $\beta_{ik}$ должны быть такими, чтобы выполнялось условие:
\[
\Cov(X_i^*, X_j) = \begin{cases}
1, \text{ при } i = j \\
0, \text{ при } i\neq j \\
\end{cases}
\]

Первый смысл коэффициента $\beta_{ik}$ — вес, с которым $X_k$ входит в величину $X_i^*$. Заметим, что есть и другая интерпретация, а именно, эти коэффициенты показывают ковариацию между новыми $X_i^*$:

\[
\Cov(X_i^*, X_j^*)= \Cov(X_i^*, \sum_{k} \beta_{jk} X_k) = \beta_{ji}
\]

Из этого, в частности, следует, что $\beta_{ij}=\beta_{ji}=\Cov(X_i^*, X_j^*)$ и $\beta_{ii} = \Var(X_i^*)$.


Из условия:
\[
X_i^* = \sum_{k=1}^n \beta_{ik} X_k
\]

Следует также представление:
\[
X_i = \sum_{k \neq i} - \frac{\beta_{ik}}{\beta_{ii}}X_k  + \frac{1}{\beta_{ii}}X_i^*
\]

То есть $\frac{1}{\beta_{ii}}X_i^*$ — это остаток от регрессии $X_i$ на остальные $X_j$. Заметим попутно, что дисперсия этого остатка равна 
\[
\Var\left( \frac{1}{\beta_{ii}}X_i^* \right) = \frac{\beta_{ii}}{\beta_{ii}^2} = \frac{1}{\beta_{ii}}
\]


Выпишем, как связаны коэффициенты $\beta_{ij}$, ковариации между новыми $X_i^*$, со ковариациями между $X_i$: 
\begin{equation}
\Cov(X_i^*, X_j) = \sum_{k=1}^n \beta_{ik}\Cov(X_k, X_j) = \begin{cases}
1, \text{ при } i = j \\
0, \text{ при } i\neq j \\
\end{cases}
\end{equation}


В матричном виде это же соотношение записывается потрясающе просто:
\[
B\cdot C = I
\]
То есть $B=C^{-1}$. Таким образом получается, что обратная ковариационная матрица позволяет легко посчитать коэффициенты любой регрессии! 


Например, мы хотим посчитать коэффициенты в регрессии $X_i$ на остальные иксы:
\[
X_i = \gamma_{i1} X_1 + \gamma_{i2} X_2 + \ldots + \gamma_{in} X_n + u_i
\]

Оказывается, что 
\[
\gamma_{ik} = -\frac{\beta_{ik}}{\beta_{ii}} = - \frac{C^{-1}_{ik}}{C^{-1}_{ii}}
\]
Кроме того, дисперсия остатка равна
\[
\Var(u_i) = \frac{1}{\beta_{ii}} = \frac{1}{C^{-1}_{ii}}
\]





Осталось заметить, что 
\[
\pCorr(X_i, X_j | ... ) = -\frac{\Cov(X_i^*, X_j^*)}{\sqrt{\Var(X_i^*)\Var(X_j^*)}} = -\frac{\beta_{ij}}{\sqrt{\beta_{ii}\beta_{jj}}}= -\frac{C^{-1}_{ij}}{\sqrt{C^{-1}_{ii}C^{-1}_{jj}}}
\]

Почему не срабатывает при $i=j$?


здесь геометрическое доказательство разных знаков!!!

Тогда обратная ковариационная матрица имеет следующую структуру:


\[
C^{-1} = \begin{pmatrix}
\frac{1}{\pVar(X_1|X_{-1})} & -\frac{\gamma_{12}}{\pVar(X_1|X_{-1})} & \\
-\frac{\gamma_{21}}{\pVar(X_1|X_{-1})} & \frac{1}{\pVar(X_2|X_{-2})} & \\
 & & \frac{1}{\pVar(X_3|X_{-3})} \\
\end{pmatrix},
\]
Где $\gamma_{ik}$ — коэффициент перед $X_k$ в регрессии $X_i$ на остальные иксы, а $\pVar(X_1|X_{-1})$ — дисперсия остатка в той же регрессии.


Похожий результат верен и для обратной корреляционной матрицы. Если $A$ — корреляционная матрица, то:

\[
A^{-1} =
\begin{pmatrix}
\frac{\Var(X_1)}{\pVar(X_1|X_{-1})} & & \\
 & \frac{\Var(X_2)}{\pVar(X_2|X_{-2})} & \\
 & & \frac{\Var(X_3)}{\pVar(X_3|X_{-3})} \\
\end{pmatrix}
\]


Частная корреляция между $X_i$ и $X_j$ при фиксированных остальных иксах равна:
\[
\pCorr(X_i, X_j| \ldots ) = -\frac{A_{ij}}{\sqrt{A_{ii}A_{jj}}}
\]

Стандартизованный коэффициент при $X_j$ в регрессии $X_i$ на остальные иксы равен:
\[
\hat\beta_{ij}^{st} = -\frac{A_{ij}}{A_{ii}}
\]

Квадрат коэффициента множественной корреляции $X_i$ с остальными иксами равен:
\[
\canCorr(X_i; \ldots ) = 1 - \frac{1}{A_{ii}}
\]



\section{Частные корреляции бывают любые}

Произвольная матрица не может быть корреляционной матрицей вектора случайных величин. Она должна быть симметричной и неотрицательно определённой, а на диагонали должны стоять единицы.

Однако произвольная симметричная матрица из чисел в диапазоне $(-1; 1)$ может быть частной корреляционной матрицей. Более того, у стационарного процессам может быть совершенная любая частная автокорреляционная функция со значениями в диапазоне $(-1;1)$. Например, у стационарного процесса может быть  частная автокорреляционная функция
\[
\phi_{kk} = \begin{cases}
0, \text{ если } k = 1 \\
0.99, \text{ если } k = 2 \\
0, \text{ если } k \geq 3 \\
\end{cases}
\]

А вот и он, $y_t = 0.99 y_{t-2} + \varepsilon_t$:

<<"ar_0_099", message=FALSE>>=
library("forecast")
y <- arima.sim(n = 1000, model = list(ar = c(0, 0.99)))
tsdisplay(y)
@



\section{Почиташки}

\url{https://en.wikipedia.org/wiki/Schur_complement#Applications_to_probability_theory_and_statistics}

\url{https://stats.stackexchange.com/questions/140080/why-does-inversion-of-a-covariance-matrix-yield-partial-correlations-between-ran}

\url{https://stats.stackexchange.com/questions/10795/how-to-interpret-an-inverse-covariance-or-precision-matrix}

\url{http://epublications.bond.edu.au/cgi/viewcontent.cgi?article=1147&context=ejsie}

\url{https://cran.r-project.org/web/packages/corpcor/corpcor.pdf}

\url{http://projecteuclid.org/euclid.aos/1176342881}

\url{http://www.gragusa.org/teaching/et/files/matrix_notes.pdf}


\url{http://www.stata-journal.com/sjpdf.html?articlenum=st0285}


\url{https://www.hse.ru/data/2016/02/15/1140408320/WP2_2016_01_F.pdf}

\url{http://www.ewi.tudelft.nl/fileadmin/Faculteit/EWI/Over_de_faculteit/Afdelingen/Applied_Mathematics/Risico_en_Beslissings_Analyse/Theses/MWawrzyniak_thesis.pdf}

\url{http://www.ewi.tudelft.nl/fileadmin/Faculteit/EWI/Over_de_faculteit/Afdelingen/Applied_Mathematics/Risico_en_Beslissings_Analyse/Theses/MWawrzyniak_thesis.pdf}

\url{http://iroha.scitech.lib.keio.ac.jp:8080/sigma/bitstream/handle/10721/1899/document.pdf?sequence=4}

\section{Выборочные характеристики}


это в статью два?



В теории обычную корреляцию и частную корреляцию можно посчитать, если известен закон распределения случайных величин. На практике закон распределения не известен, однако доступны наблюдения. Как по имеющимся наблюдениям оценить неизвестные корреляции?


Несколько способов оценки корреляции


Здесь пара картинок: википедийная с корреляциями и два ряда случайного блуждания/тренда



Несколько способов оценки частной корреляции


Оценкой для частной дисперсии $X_1$ при фиксированных $X_2$, \ldots, $X_n$ является оценка дисперсии случайной составляющей в регрессии $X_1$ на $X_2$, \ldots, $X_n$


А ещё частную корреляцию можно посчитать в два шага.
Построить регрессию $X_1$ на остальные переменные. Взять $t$-статистику перед $X_2$ и воспользоваться формулой:
\[
\pCorr(X_i, X_j | X_{-i,-j}) = \frac{t^2}{t^2 + (n-k)}
\]



Здесь про зависимости возникающие при оценке автокорреляционной и частной автокорреляционной функции для случайных процессов.


Несколько способов оценки множественной корреляции:

Если $\canCorr(Y; X_1, X_2, \ldots, X_k) = 0$, то $\E(R^2_{adj})=0$.


Обратная к выборочной ковариационной матрице

\[
\hat C^{-1} = \begin{pmatrix}
\frac{1}{\hat \sigma^2_1} & & \\
 & \frac{1}{\hat \sigma^2_2} & \\ 
 & & \frac{1}{\hat \sigma^2_3} \\
\end{pmatrix}
\]

Если мы предположим, что разные наблюдения в выборке независимы и одинаково распределены, то получим такие выборочные оценки

\begin{tabular}{lll} \toprule
Теоретический & Выборочный & Коммент \\
\midrule
$\pVar(y_i|z_i)$      & $\hat \sigma_u^2$ в регрессии $y$ на $z$, выборочная дисперсия остатков & несмещённость? \\
$\pCov(y_i, x_i|z_i)$ & выборочная ковариация остатков двух регрессий & несмещённость \\
$\canCorr(y_i; x_{i1}, x_{i2}, \ldots, x_{ik})$ & $R^2$, $R^2_{adj}$ &  \\
коэффициент в множественной регрессии & $\hb = (X'X)^{-1}X'y$ & несмещенный \\
стандартизированный коэффициент в множественной регрессии & $\hb_j^{st}=\hb_j \frac{\hat\sigma_x}{\hat\sigma_y}$, $\hb^{st} = (X^{st\prime}X^{st})^{-1}X^{st\prime}y^{st}$ & несмещенный? \\
обычная корреляция & $sCorr(y, x) = \frac{\sum (x_i - \bar x)(y_i - \bar y)}{\sqrt{...}}$ & \\
частная корреляция & корреляция остатков регрессии, обращение корреляционной матрицы & \\
\bottomrule
\end{tabular}

\section{здесь должно быть про VIF!}



\end{document}
